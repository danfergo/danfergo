{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/huxblog/source/css/bootstrap.css","path":"css/bootstrap.css","modified":0,"renderable":1},{"_id":"themes/huxblog/source/css/bootstrap.min.css","path":"css/bootstrap.min.css","modified":0,"renderable":1},{"_id":"themes/huxblog/source/css/hamburgers.min.css","path":"css/hamburgers.min.css","modified":0,"renderable":1},{"_id":"themes/huxblog/source/css/highlight.styl","path":"css/highlight.styl","modified":0,"renderable":1},{"_id":"themes/huxblog/source/css/hux-blog.css","path":"css/hux-blog.css","modified":1,"renderable":1},{"_id":"themes/huxblog/source/fonts/glyphicons-halflings-regular.eot","path":"fonts/glyphicons-halflings-regular.eot","modified":0,"renderable":1},{"_id":"themes/huxblog/source/fonts/glyphicons-halflings-regular.svg","path":"fonts/glyphicons-halflings-regular.svg","modified":0,"renderable":1},{"_id":"themes/huxblog/source/fonts/glyphicons-halflings-regular.ttf","path":"fonts/glyphicons-halflings-regular.ttf","modified":0,"renderable":1},{"_id":"themes/huxblog/source/fonts/glyphicons-halflings-regular.woff","path":"fonts/glyphicons-halflings-regular.woff","modified":0,"renderable":1},{"_id":"themes/huxblog/source/fonts/glyphicons-halflings-regular.woff2","path":"fonts/glyphicons-halflings-regular.woff2","modified":0,"renderable":1},{"_id":"themes/huxblog/source/js/bootstrap.js","path":"js/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/huxblog/source/js/bootstrap.min.js","path":"js/bootstrap.min.js","modified":0,"renderable":1},{"_id":"themes/huxblog/source/js/hux-blog.js","path":"js/hux-blog.js","modified":0,"renderable":1},{"_id":"themes/huxblog/source/js/hux-blog.min.js","path":"js/hux-blog.min.js","modified":0,"renderable":1},{"_id":"themes/huxblog/source/js/jquery.js","path":"js/jquery.js","modified":0,"renderable":1},{"_id":"themes/huxblog/source/js/jquery.min.js","path":"js/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/huxblog/source/js/jquery.nav.js","path":"js/jquery.nav.js","modified":0,"renderable":1},{"_id":"themes/huxblog/source/js/jquery.tagcloud.js","path":"js/jquery.tagcloud.js","modified":0,"renderable":1},{"_id":"source/img/avatar.jpg","path":"img/avatar.jpg","modified":0,"renderable":0},{"_id":"source/img/favicon.png","path":"img/favicon.png","modified":0,"renderable":0},{"_id":"source/img/home-bg.jpg","path":"img/home-bg.jpg","modified":0,"renderable":0}],"Cache":[{"_id":"themes/huxblog/README.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1590873707821},{"_id":"source/404.md","hash":"c57e66354d76c5a1c99e3c84b03f0f70b1983e1a","modified":1590873707805},{"_id":"source/_discarded/Custom-image-generator-for-Keras.md","hash":"3c317fb2bb198de0fc025cc38c7b20cf07246568","modified":1590873707809},{"_id":"source/_posts/clevo-p65xrp-inphtech-p600-g-on-linux.md","hash":"5996d8d5341c4f9d4b87a0b10b5e05a00080e465","modified":1590884784851},{"_id":"source/_posts/garmnet.md","hash":"c8b1cf62e95c50d0eca6013f2cea67e19baa177e","modified":1590884446840},{"_id":"source/_posts/gelsight-simulation.md","hash":"0036eb2e9c40b6e618e5104016d5e654cdf65c6a","modified":1611104971435},{"_id":"source/_posts/geltip.md","hash":"ca854f466f1c6bb90a851ad92ba9a321ead69122","modified":1611104971451},{"_id":"source/_posts/hello-world.md","hash":"d5a32fd2ec3cea610c0525c706ba952aee1c7aab","modified":1590884412652},{"_id":"source/about/index.md","hash":"6f2aebfeff9aeb02c93def0fc53fb87edc45e80d","modified":1590882499861},{"_id":"source/img/avatar.jpg","hash":"d03aadf9351e37af606034c8aaef2f658e0a9044","modified":1590873707809},{"_id":"source/img/favicon.png","hash":"ccbea0c393db5ed5c9e92db54a1dbb42629454d5","modified":1590873707809},{"_id":"source/tags/index.md","hash":"8f6a80fbe9a4f0d7d02a92e44c1de308adba753b","modified":1590873707821},{"_id":"source/_posts/clevo-p65xrp-inphtech-p600-g-on-linux/clevo-linux.jpg","hash":"f4ef8a595f3875d41dc59b8ad4057d2ec5470742","modified":1590873707809},{"_id":"source/_posts/garmnet/cover.png","hash":"7ec64254a7143090bead3166b4d7e64f77c47115","modified":1590878296029},{"_id":"source/_posts/gelsight-simulation/heightmap.jpg","hash":"c7ba7299249b370b70bc3b98d4e4afd39e7fa568","modified":1611074525454},{"_id":"source/_posts/geltip/cover.jpg","hash":"818c864731bbda7ef40c9c9e9be8df1cc27d4c35","modified":1590880818167},{"_id":"source/_posts/geltip/objects_dataset.zip","hash":"e9ecd8512e646714dcecab58f485f901683cbbdf","modified":1595705107575},{"_id":"themes/huxblog/LICENSE","hash":"2b209f06bebeb2a8c2b7e187e436f3e1e1fbc8a7","modified":1590873707821},{"_id":"themes/huxblog/_config.yml","hash":"5d235aa7716657282795ee86d9c9db2374197a66","modified":1590873707821},{"_id":"themes/huxblog/languages_to_be_added/de.yml","hash":"424a9c1e6ab69334d7873f6574da02ca960aa572","modified":1590873707821},{"_id":"themes/huxblog/languages_to_be_added/default.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1590873707821},{"_id":"themes/huxblog/languages_to_be_added/en.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1590873707821},{"_id":"themes/huxblog/languages_to_be_added/es.yml","hash":"cb4eeca0ed3768a77e0cd216300f2b2549628b1b","modified":1590873707821},{"_id":"themes/huxblog/languages_to_be_added/no.yml","hash":"8ca475a3b4f8efe6603030f0013aae39668230e1","modified":1590873707821},{"_id":"themes/huxblog/languages_to_be_added/pl.yml","hash":"de7eb5850ae65ba7638e907c805fea90617a988c","modified":1590873707821},{"_id":"themes/huxblog/languages_to_be_added/ru.yml","hash":"42df7afeb7a35dc46d272b7f4fb880a9d9ebcaa5","modified":1590873707821},{"_id":"themes/huxblog/languages_to_be_added/zh-CN.yml","hash":"7bfcb0b8e97d7e5edcfca8ab26d55d9da2573c1c","modified":1590873707821},{"_id":"themes/huxblog/languages_to_be_added/zh-TW.yml","hash":"9acac6cc4f8002c3fa53ff69fb8cf66c915bd016","modified":1590873707821},{"_id":"themes/huxblog/layout/404.ejs","hash":"7878b25ee8c8b5dff6c5afc5f544ce4247fb0810","modified":1590873707821},{"_id":"themes/huxblog/layout/about.ejs","hash":"19627177f4f4a57c6c7f9ba8f44d50c7d299ab41","modified":1590873707821},{"_id":"themes/huxblog/layout/archive.ejs","hash":"6c3ed5d914379319efe835a4aa505abbc616c328","modified":1590873707821},{"_id":"themes/huxblog/layout/archives.ejs","hash":"f0046e58cc1dd876133be2bf927aed2b1821cb3e","modified":1590873707821},{"_id":"themes/huxblog/layout/index.ejs","hash":"96a39562c7189f3f84c7e63fd943bcd4765f594e","modified":1590880361668},{"_id":"themes/huxblog/layout/keynote.ejs","hash":"16a4164c3d291131fb66078e8df93b31fe050040","modified":1590873707821},{"_id":"themes/huxblog/layout/layout.ejs","hash":"b1304bc5eae41d93acacd80590d4cf2fd40632fc","modified":1595689226490},{"_id":"themes/huxblog/layout/page.ejs","hash":"39fe389be648e744135a219737258bc36fba2a35","modified":1590873707821},{"_id":"themes/huxblog/layout/post.ejs","hash":"d46c02f8863f8e823b38823e4a7ebf42533a5644","modified":1590873707821},{"_id":"themes/huxblog/layout/tags.ejs","hash":"f603d7341f4dd83fbef82053994b894989550697","modified":1590873707821},{"_id":"themes/huxblog/layout/_partial/footer.ejs","hash":"85068ce421b330d3ff515f85414e12c641747c8c","modified":1595691245444},{"_id":"themes/huxblog/layout/_partial/head.ejs","hash":"ac4d50065ef0a235a0a21b7845b6b49598893865","modified":1590873707821},{"_id":"themes/huxblog/layout/_partial/nav.ejs","hash":"fa681a37e190152c6ffd6822123f9ee4e9eaa83d","modified":1590873707821},{"_id":"themes/huxblog/layout/_partial/pagination.ejs","hash":"557d6bb069a1d48af49ae912994653f44b32a570","modified":1590873707821},{"_id":"themes/huxblog/layout/_partial/sidebar.ejs","hash":"6536577887af2b21430824ed87381df26e9eef01","modified":1590873707821},{"_id":"themes/huxblog/layout/_partial/social.ejs","hash":"1228b7efceeecff13db9a087bd07aa4888e23824","modified":1590873707821},{"_id":"themes/huxblog/source/css/hamburgers.min.css","hash":"c880d5f09f702e5a382dbceed8d4efe7cc09e884","modified":1590873707825},{"_id":"themes/huxblog/source/css/highlight.styl","hash":"e842080e6d580f0f70a7df71fbde3c4e49463c19","modified":1590873707825},{"_id":"themes/huxblog/source/css/hux-blog.css","hash":"6f6e498110fc419f046a84a4387606fe777c25d4","modified":1611073659959},{"_id":"themes/huxblog/source/fonts/glyphicons-halflings-regular.eot","hash":"86b6f62b7853e67d3e635f6512a5a5efc58ea3c3","modified":1590873707825},{"_id":"themes/huxblog/source/fonts/glyphicons-halflings-regular.ttf","hash":"44bc1850f570972267b169ae18f1cb06b611ffa2","modified":1590873707825},{"_id":"themes/huxblog/source/fonts/glyphicons-halflings-regular.woff","hash":"278e49a86e634da6f2a02f3b47dd9d2a8f26210f","modified":1590873707825},{"_id":"themes/huxblog/source/fonts/glyphicons-halflings-regular.woff2","hash":"ca35b697d99cae4d1b60f2d60fcd37771987eb07","modified":1590873707825},{"_id":"themes/huxblog/source/js/bootstrap.min.js","hash":"b3f2ef9f985e7906c9360756b73cd64bf7733647","modified":1590873707825},{"_id":"themes/huxblog/source/js/hux-blog.js","hash":"182457870e24353f0ac039ce32cf976d10375f94","modified":1590873707825},{"_id":"themes/huxblog/source/js/hux-blog.min.js","hash":"1563e7f70550ac6b30803d6f449719b853200e35","modified":1590873707825},{"_id":"themes/huxblog/source/js/jquery.nav.js","hash":"ef2160a456176a4d09cc0b95d52b27dfbbadf2d8","modified":1590873707825},{"_id":"themes/huxblog/source/js/jquery.tagcloud.js","hash":"4e5fd0b07f3bd935f2e603710447e039e3677211","modified":1590873707829},{"_id":"source/_posts/gelsight-simulation/object_set.jpg","hash":"688af3493c08a4c733656f8269eda3739c6157a6","modified":1610991137860},{"_id":"source/_posts/gelsight-simulation/texture_augmented.jpg","hash":"ab2fadd88e332ff6dd41f058383d5d8fa891e2d3","modified":1610991179804},{"_id":"source/_posts/gelsight-simulation/texture_perturbances.jpg","hash":"ab2fadd88e332ff6dd41f058383d5d8fa891e2d3","modified":1610991167812},{"_id":"source/_posts/geltip/projective_model.mp4","hash":"83781258c1749488600472206c5974f3def70468","modified":1595619641409},{"_id":"themes/huxblog/source/css/bootstrap.min.css","hash":"973e37a8502921d56bc02bb55321f45b072b6f71","modified":1590873707825},{"_id":"themes/huxblog/source/fonts/glyphicons-halflings-regular.svg","hash":"de51a8494180a6db074af2dee2383f0a363c5b08","modified":1590873707825},{"_id":"themes/huxblog/source/js/bootstrap.js","hash":"f8752e9ae24daec0a0baffd7819122f8c6fd9103","modified":1590873707825},{"_id":"themes/huxblog/source/js/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1590873707825},{"_id":"source/_posts/gelsight-simulation/cover.png","hash":"083605233c7e3213f8b8313137581d7a140852d5","modified":1590880161696},{"_id":"themes/huxblog/source/css/bootstrap.css","hash":"41c54bf695145ae0b4d9020a1da308ceb05dcaf3","modified":1590873707821},{"_id":"themes/huxblog/source/js/jquery.js","hash":"1852661bd11a09ca9b9cb63d1aa6ff390fffaf4e","modified":1590873707825},{"_id":"source/_posts/geltip/geltip2020_parts.zip","hash":"283d126175afcd49b311da2f0b934bd72d890c39","modified":1595687445876},{"_id":"source/_posts/gelsight-simulation/vitac2019workshop.jpg","hash":"1a572237310a7b955ef0b4fe6740da3afc94bf5b","modified":1610990939306},{"_id":"source/_posts/garmnet/img_spacial_constraint.png","hash":"7b69e41524af221c84ab342f458f648f1f6352f2","modified":1590878296045},{"_id":"source/_posts/gelsight-simulation/samples_qualitative.jpg","hash":"3cad9302eb8a28ec2feda36bbb7dfde26392feab","modified":1610868904653},{"_id":"source/_posts/gelsight-simulation/samples.png","hash":"db5a4c847a5612c7ecaaf7e7d6982f12db6bb933","modified":1590882059390},{"_id":"source/img/home-bg.jpg","hash":"32c8c98e54e6e0c0fa8fb344809cab37242458f6","modified":1590873707821},{"_id":"source/_posts/gelsight-simulation/envs.png","hash":"30c41f030630fff01fb7094c9efeea03f9308469","modified":1590879636121},{"_id":"source/_posts/geltip/exp_world_blocks.mp4","hash":"716974b30e1a85b36cc9c53418234f46ec26a45a","modified":1590881122969},{"_id":"source/_posts/geltip/exp_contact_detection.mp4","hash":"b1e984cec4d20f0bfc2b4e84c70d0c80b5addfb9","modified":1590881124561},{"_id":"source/_posts/gelsight-simulation/data_collection.webm","hash":"1088314f7d57c4f3d6f75240b7c442fca953b25c","modified":1590871078249},{"_id":"public/2016/10/17/clevo-p65xrp-inphtech-p600-g-on-linux/index.html","hash":"6201814f79d8815309bb956c9fa9c5a4c31d4fad","modified":1611105006870},{"_id":"public/2016/10/14/hello-world/index.html","hash":"359a2b562af480c2bf427b899260326a941ddd9f","modified":1611105006870},{"_id":"public/atom.xml","hash":"2225f53dbc016d60719ae6b0d77a206c563f48ad","modified":1611105006870},{"_id":"public/404.html","hash":"e4b20126ef72803139aadf36f6bbdaaff97084e6","modified":1611105006870},{"_id":"public/archives/index.html","hash":"1b1a063224c86f92f639529004eb51da7021e701","modified":1611105006870},{"_id":"public/archives/2016/index.html","hash":"f9d2475bf52171096df00b608841b9fb7aecb84c","modified":1611105006870},{"_id":"public/archives/2016/10/index.html","hash":"374de0c1343c05155e6deec66f8ae554f8e23fc6","modified":1611105006870},{"_id":"public/archives/2019/index.html","hash":"bcb16a85ef662ebc7cfa5181999046fd02cb6c2f","modified":1611105006870},{"_id":"public/archives/2019/03/index.html","hash":"5bb89a87254a9f1197bf3125d2052422fdc93d15","modified":1611105006870},{"_id":"public/archives/2019/05/index.html","hash":"0d735c07719f398c8c96f8beac866debb6f10842","modified":1611105006870},{"_id":"public/archives/2020/index.html","hash":"91022b6d120155a8b8fa2ac44518bb60dade3559","modified":1611105006870},{"_id":"public/archives/2020/03/index.html","hash":"fd5651f6bbe608285d429af56aa36fbf7df8a7e8","modified":1611105006870},{"_id":"public/tags/featured/index.html","hash":"753eacfc548614f406fc8ab4f5788d30119246fc","modified":1611105006870},{"_id":"public/tags/vision/index.html","hash":"ad7fe2b16ff600045bf2f66e5e15413d0466e315","modified":1611105006870},{"_id":"public/tags/linux/index.html","hash":"07de17cc9d073b340313130d85cb82f9fcd85002","modified":1611105006870},{"_id":"public/tags/touch/index.html","hash":"0a07e11863a6b548f1b7ef47c4d5b258bca8d7e4","modified":1611105006870},{"_id":"public/tags/meta/index.html","hash":"fb44bbe9034e553f6bbf68151f631683a9584dee","modified":1611105006870},{"_id":"public/tags/index.html","hash":"96cbfb6520b64bcb421612fee55366b53fcd10b0","modified":1611105006870},{"_id":"public/geltip/index.html","hash":"62e8532e871472e81fc9e2ba2825bd66ef61d7b3","modified":1611105006870},{"_id":"public/gelsight-simulation/index.html","hash":"66deb32dd6f72f0bb24cade2bf63be735ccc36e1","modified":1611105006870},{"_id":"public/garmnet/index.html","hash":"55824c150a03e87334b8dbc1f725555fdc5081b7","modified":1611105006870},{"_id":"public/clevo-p65xrp-inphtech-p600-g-on-linux/index.html","hash":"475b2fe91056dfe765f6c7ad52a8c7b800123a8b","modified":1611105006870},{"_id":"public/hello-world/index.html","hash":"221d392223388146044462e61ca0e382e4bdf5c7","modified":1611105006870},{"_id":"public/about/index.html","hash":"0edd3db84b3eb9a6409135b9ca3df175202ba53b","modified":1611105006870},{"_id":"public/index.html","hash":"d6f17902d6be8b7ae46c0c56d75cf2ced8dcbdac","modified":1611105006870},{"_id":"public/fonts/glyphicons-halflings-regular.eot","hash":"86b6f62b7853e67d3e635f6512a5a5efc58ea3c3","modified":1611105006870},{"_id":"public/fonts/glyphicons-halflings-regular.ttf","hash":"44bc1850f570972267b169ae18f1cb06b611ffa2","modified":1611105006870},{"_id":"public/fonts/glyphicons-halflings-regular.woff","hash":"278e49a86e634da6f2a02f3b47dd9d2a8f26210f","modified":1611105006870},{"_id":"public/fonts/glyphicons-halflings-regular.woff2","hash":"ca35b697d99cae4d1b60f2d60fcd37771987eb07","modified":1611105006870},{"_id":"public/img/avatar.jpg","hash":"d03aadf9351e37af606034c8aaef2f658e0a9044","modified":1611105006870},{"_id":"public/img/favicon.png","hash":"ccbea0c393db5ed5c9e92db54a1dbb42629454d5","modified":1611105006870},{"_id":"public/garmnet/cover.png","hash":"7ec64254a7143090bead3166b4d7e64f77c47115","modified":1611105006870},{"_id":"public/clevo-p65xrp-inphtech-p600-g-on-linux/clevo-linux.jpg","hash":"f4ef8a595f3875d41dc59b8ad4057d2ec5470742","modified":1611105006870},{"_id":"public/gelsight-simulation/heightmap.jpg","hash":"c7ba7299249b370b70bc3b98d4e4afd39e7fa568","modified":1611105006870},{"_id":"public/geltip/cover.jpg","hash":"818c864731bbda7ef40c9c9e9be8df1cc27d4c35","modified":1611105006870},{"_id":"public/geltip/objects_dataset.zip","hash":"e9ecd8512e646714dcecab58f485f901683cbbdf","modified":1611105006870},{"_id":"public/fonts/glyphicons-halflings-regular.svg","hash":"de51a8494180a6db074af2dee2383f0a363c5b08","modified":1611105006870},{"_id":"public/gelsight-simulation/object_set.jpg","hash":"688af3493c08a4c733656f8269eda3739c6157a6","modified":1611105006870},{"_id":"public/gelsight-simulation/texture_augmented.jpg","hash":"ab2fadd88e332ff6dd41f058383d5d8fa891e2d3","modified":1611105006870},{"_id":"public/gelsight-simulation/texture_perturbances.jpg","hash":"ab2fadd88e332ff6dd41f058383d5d8fa891e2d3","modified":1611105006870},{"_id":"public/geltip/projective_model.mp4","hash":"83781258c1749488600472206c5974f3def70468","modified":1611105006870},{"_id":"public/gelsight-simulation/cover.png","hash":"083605233c7e3213f8b8313137581d7a140852d5","modified":1611105006870},{"_id":"public/css/highlight.css","hash":"8bc5e670b028eda8097f58bdc85269c5124c2951","modified":1611105006870},{"_id":"public/js/hux-blog.js","hash":"182457870e24353f0ac039ce32cf976d10375f94","modified":1611105006870},{"_id":"public/js/hux-blog.min.js","hash":"1563e7f70550ac6b30803d6f449719b853200e35","modified":1611105006870},{"_id":"public/js/jquery.nav.js","hash":"ef2160a456176a4d09cc0b95d52b27dfbbadf2d8","modified":1611105006870},{"_id":"public/js/jquery.tagcloud.js","hash":"4e5fd0b07f3bd935f2e603710447e039e3677211","modified":1611105006870},{"_id":"public/css/hamburgers.min.css","hash":"c880d5f09f702e5a382dbceed8d4efe7cc09e884","modified":1611105006870},{"_id":"public/css/hux-blog.css","hash":"6f6e498110fc419f046a84a4387606fe777c25d4","modified":1611105006870},{"_id":"public/geltip/geltip2020_parts.zip","hash":"283d126175afcd49b311da2f0b934bd72d890c39","modified":1611105006870},{"_id":"public/js/bootstrap.min.js","hash":"b3f2ef9f985e7906c9360756b73cd64bf7733647","modified":1611105006870},{"_id":"public/gelsight-simulation/vitac2019workshop.jpg","hash":"1a572237310a7b955ef0b4fe6740da3afc94bf5b","modified":1611105006870},{"_id":"public/js/bootstrap.js","hash":"f8752e9ae24daec0a0baffd7819122f8c6fd9103","modified":1611105006870},{"_id":"public/js/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1611105006870},{"_id":"public/garmnet/img_spacial_constraint.png","hash":"7b69e41524af221c84ab342f458f648f1f6352f2","modified":1611105006870},{"_id":"public/css/bootstrap.min.css","hash":"973e37a8502921d56bc02bb55321f45b072b6f71","modified":1611105006870},{"_id":"public/css/bootstrap.css","hash":"41c54bf695145ae0b4d9020a1da308ceb05dcaf3","modified":1611105006870},{"_id":"public/gelsight-simulation/samples_qualitative.jpg","hash":"3cad9302eb8a28ec2feda36bbb7dfde26392feab","modified":1611105006870},{"_id":"public/js/jquery.js","hash":"1852661bd11a09ca9b9cb63d1aa6ff390fffaf4e","modified":1611105006870},{"_id":"public/gelsight-simulation/samples.png","hash":"db5a4c847a5612c7ecaaf7e7d6982f12db6bb933","modified":1611105006870},{"_id":"public/img/home-bg.jpg","hash":"32c8c98e54e6e0c0fa8fb344809cab37242458f6","modified":1611105006870},{"_id":"public/gelsight-simulation/envs.png","hash":"30c41f030630fff01fb7094c9efeea03f9308469","modified":1611105006870},{"_id":"public/geltip/exp_world_blocks.mp4","hash":"716974b30e1a85b36cc9c53418234f46ec26a45a","modified":1611105006870},{"_id":"public/geltip/exp_contact_detection.mp4","hash":"b1e984cec4d20f0bfc2b4e84c70d0c80b5addfb9","modified":1611105006870},{"_id":"public/gelsight-simulation/data_collection.webm","hash":"1088314f7d57c4f3d6f75240b7c442fca953b25c","modified":1611105006870}],"Category":[],"Data":[],"Page":[{"layout":"404","subtitle":"Oops! That page doesn't exist.","date":"2017-03-24T19:48:55.000Z","_content":"","source":"404.md","raw":"layout: 404\nsubtitle: Oops! That page doesn't exist.\ndate: 2017-03-24 19:48:55\n---\n","updated":"2020-05-30T21:21:47.805Z","path":"404.html","title":"","comments":1,"_id":"ckk4qbnqr0000huvpel266fad","content":"<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css\">","site":{"data":{}},"excerpt":"","more":""},{"layout":"tags","subtitle":"Oops! That page doesn't exist.","title":"Tags","date":"2017-03-24T19:48:55.000Z","_content":"","source":"tags/index.md","raw":"layout: tags\nsubtitle: Oops! That page doesn't exist.\ntitle: Tags\ndate: 2017-03-24 19:48:55\n---\n","updated":"2020-05-30T21:21:47.821Z","path":"tags/index.html","comments":1,"_id":"ckk4qbnqx0002huvp21fpefuv","content":"<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css\">","site":{"data":{}},"excerpt":"","more":""},{"layout":"about","title":"About","subtitle":"A compressed view of who I am and what I do.","date":"2016-10-14T03:08:01.000Z","_content":"Hi!,\n\nMy name is Daniel Fernandes Gomes, I was born in 94 and I’m from S. Paio de Oleiros; a northen, near the coast, Portuguese town. Though, many times you may also find me using my alias, Danfergo, which is the agglutination of my full name.\n\nMy memories begin with me launching my little toy cars against the wall and playing with everything else I could find at home. Tying chairs and tables with wool yarn was a trend back then.\n\nAround eight, I’ve my bike's tire flattened and from their on that was my main toy. I’ve self-learned how to take it apart, fix it and a few years later reinventing it as a tricycle with a steering wheel from an old car.\n\nAt this age, I was studying at the elementary schools of S. Paio de Oleiros and Santa Maria de Lamas (latest year) while attending \"sunday school\", which I kept attending until my 9th grade.\n\nThe next studying cycles happened at Colégio Liceal de Santa Maria de Lamas, where I was doing enough to keep reasonable grades while I would be joining a few clubs, in different years: history, english and arts. A lot more seriously, I’ve joined theatre group where I stayed until the end of the high-school, performing during 5 and half years, around 10 different plays.\n\nAt 15, when the social network Hi5 was at its pick, I’ve found the CSS “codes” and browser source view. I was amazed by It and I’ve embarked on this new learning through hacking path. About a year latter I was publishing tutorials on Youtube and on TOUTLOUD, a forum which I created with the help of a few friends and reached around 2500 users. This motivated me to learn my first programming languages: JavaScript and Php. The forum died when the social network disabled HTML/CSS support.\n\nAround this age I joined the handball teams, at school and at CDC S. Paio de Oleiros though, for logistic reasons It didn’t last one full year. Next I took on classic guitar lessons at TUNA Musical Group from S. Paio de Oleiros, for two years. This led me to, with friends, create COCKAINE,a rock music band. I left after a few months.\n\nSince early, I’ve questioned every conception it was purposed to me — that’s stupid, was my usual welcome message. But more than put in question I love to propose, dream about and build my own ideas. Specially for problems that even after long periods of reflection keep staying stupid.\n\nAt my ninth grade I started questioning students association and that led me to, at eleventh, with the help of friends, run for president. Why not?… And we totally lost. But with the lessons learned and contacts established we decided to run once again. We won. I was the president of students association that year; and what a year we: organized two major parties (w/ crowds over 1500 people), a LAN Party sponsored by the town hall and an ISP provider, provided logistic support and were the representatives on the national secondary festival, as well other smaller events.\n\nFrom early I wanted to be an engineer and he only options where what branch: mechanical, electrical, computers, etc. Though, three main points quickly help me take that decision:\n\n1. My background with web programming and how much I liked it.\n2. I spent a lot of time thinking about existential questions such as life and death, intelligence and consciousness. I looked to computers with the same intrigue as I looked to people and figure it out that maybe by understanding better computers I could understand better people.\n3. All my craziest ideas would involve some sort of Artificial Intelligence.\n\nWhen the university came, my marks weren’t enough to enter the faculty I have initially chosen, Faculty of Engineering of University of Porto (FEUP). Yet I was able to enter a neighbour school, Institute of Engineering of Porto, where I made the first year. Then I requested transfer to FEUP were I’m making the final four years of the course Master in Informatics and Computing Engineering.\n\nMeanwhile I’ve been participating with friends, each year, on the Porto Summer Of Code competition, having conquered the second place on the first attempt. I’ve also been participating in other middle year competitions.\n\nSince the summer of 2015 I’ve also been working with university professors, under three initiation research grants, on a web platform for the support of wind flow modeling experimental campaigns. Here the technologies used were AngularJS and NodeJS based.\n\nThis a bit of who I am. At least so far.\n\nIf you think that you might have common interests with me, don’t hesitate in contacting me.\n\nThink less, talk less, do more.\nDaniel\n","source":"about/index.md","raw":"layout: about\ntitle: About\nsubtitle: A compressed view of who I am and what I do.\ndate: 2016-10-14 04:08:01\n---\nHi!,\n\nMy name is Daniel Fernandes Gomes, I was born in 94 and I’m from S. Paio de Oleiros; a northen, near the coast, Portuguese town. Though, many times you may also find me using my alias, Danfergo, which is the agglutination of my full name.\n\nMy memories begin with me launching my little toy cars against the wall and playing with everything else I could find at home. Tying chairs and tables with wool yarn was a trend back then.\n\nAround eight, I’ve my bike's tire flattened and from their on that was my main toy. I’ve self-learned how to take it apart, fix it and a few years later reinventing it as a tricycle with a steering wheel from an old car.\n\nAt this age, I was studying at the elementary schools of S. Paio de Oleiros and Santa Maria de Lamas (latest year) while attending \"sunday school\", which I kept attending until my 9th grade.\n\nThe next studying cycles happened at Colégio Liceal de Santa Maria de Lamas, where I was doing enough to keep reasonable grades while I would be joining a few clubs, in different years: history, english and arts. A lot more seriously, I’ve joined theatre group where I stayed until the end of the high-school, performing during 5 and half years, around 10 different plays.\n\nAt 15, when the social network Hi5 was at its pick, I’ve found the CSS “codes” and browser source view. I was amazed by It and I’ve embarked on this new learning through hacking path. About a year latter I was publishing tutorials on Youtube and on TOUTLOUD, a forum which I created with the help of a few friends and reached around 2500 users. This motivated me to learn my first programming languages: JavaScript and Php. The forum died when the social network disabled HTML/CSS support.\n\nAround this age I joined the handball teams, at school and at CDC S. Paio de Oleiros though, for logistic reasons It didn’t last one full year. Next I took on classic guitar lessons at TUNA Musical Group from S. Paio de Oleiros, for two years. This led me to, with friends, create COCKAINE,a rock music band. I left after a few months.\n\nSince early, I’ve questioned every conception it was purposed to me — that’s stupid, was my usual welcome message. But more than put in question I love to propose, dream about and build my own ideas. Specially for problems that even after long periods of reflection keep staying stupid.\n\nAt my ninth grade I started questioning students association and that led me to, at eleventh, with the help of friends, run for president. Why not?… And we totally lost. But with the lessons learned and contacts established we decided to run once again. We won. I was the president of students association that year; and what a year we: organized two major parties (w/ crowds over 1500 people), a LAN Party sponsored by the town hall and an ISP provider, provided logistic support and were the representatives on the national secondary festival, as well other smaller events.\n\nFrom early I wanted to be an engineer and he only options where what branch: mechanical, electrical, computers, etc. Though, three main points quickly help me take that decision:\n\n1. My background with web programming and how much I liked it.\n2. I spent a lot of time thinking about existential questions such as life and death, intelligence and consciousness. I looked to computers with the same intrigue as I looked to people and figure it out that maybe by understanding better computers I could understand better people.\n3. All my craziest ideas would involve some sort of Artificial Intelligence.\n\nWhen the university came, my marks weren’t enough to enter the faculty I have initially chosen, Faculty of Engineering of University of Porto (FEUP). Yet I was able to enter a neighbour school, Institute of Engineering of Porto, where I made the first year. Then I requested transfer to FEUP were I’m making the final four years of the course Master in Informatics and Computing Engineering.\n\nMeanwhile I’ve been participating with friends, each year, on the Porto Summer Of Code competition, having conquered the second place on the first attempt. I’ve also been participating in other middle year competitions.\n\nSince the summer of 2015 I’ve also been working with university professors, under three initiation research grants, on a web platform for the support of wind flow modeling experimental campaigns. Here the technologies used were AngularJS and NodeJS based.\n\nThis a bit of who I am. At least so far.\n\nIf you think that you might have common interests with me, don’t hesitate in contacting me.\n\nThink less, talk less, do more.\nDaniel\n","updated":"2020-05-30T23:48:19.861Z","path":"about/index.html","comments":1,"_id":"ckk4qbnr10005huvphcppft7e","content":"<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css\"><p>Hi!,</p>\n<p>My name is Daniel Fernandes Gomes, I was born in 94 and I’m from S. Paio de Oleiros; a northen, near the coast, Portuguese town. Though, many times you may also find me using my alias, Danfergo, which is the agglutination of my full name.</p>\n<p>My memories begin with me launching my little toy cars against the wall and playing with everything else I could find at home. Tying chairs and tables with wool yarn was a trend back then.</p>\n<p>Around eight, I’ve my bike’s tire flattened and from their on that was my main toy. I’ve self-learned how to take it apart, fix it and a few years later reinventing it as a tricycle with a steering wheel from an old car.</p>\n<p>At this age, I was studying at the elementary schools of S. Paio de Oleiros and Santa Maria de Lamas (latest year) while attending “sunday school”, which I kept attending until my 9th grade.</p>\n<p>The next studying cycles happened at Colégio Liceal de Santa Maria de Lamas, where I was doing enough to keep reasonable grades while I would be joining a few clubs, in different years: history, english and arts. A lot more seriously, I’ve joined theatre group where I stayed until the end of the high-school, performing during 5 and half years, around 10 different plays.</p>\n<p>At 15, when the social network Hi5 was at its pick, I’ve found the CSS “codes” and browser source view. I was amazed by It and I’ve embarked on this new learning through hacking path. About a year latter I was publishing tutorials on Youtube and on TOUTLOUD, a forum which I created with the help of a few friends and reached around 2500 users. This motivated me to learn my first programming languages: JavaScript and Php. The forum died when the social network disabled HTML/CSS support.</p>\n<p>Around this age I joined the handball teams, at school and at CDC S. Paio de Oleiros though, for logistic reasons It didn’t last one full year. Next I took on classic guitar lessons at TUNA Musical Group from S. Paio de Oleiros, for two years. This led me to, with friends, create COCKAINE,a rock music band. I left after a few months.</p>\n<p>Since early, I’ve questioned every conception it was purposed to me — that’s stupid, was my usual welcome message. But more than put in question I love to propose, dream about and build my own ideas. Specially for problems that even after long periods of reflection keep staying stupid.</p>\n<p>At my ninth grade I started questioning students association and that led me to, at eleventh, with the help of friends, run for president. Why not?… And we totally lost. But with the lessons learned and contacts established we decided to run once again. We won. I was the president of students association that year; and what a year we: organized two major parties (w/ crowds over 1500 people), a LAN Party sponsored by the town hall and an ISP provider, provided logistic support and were the representatives on the national secondary festival, as well other smaller events.</p>\n<p>From early I wanted to be an engineer and he only options where what branch: mechanical, electrical, computers, etc. Though, three main points quickly help me take that decision:</p>\n<ol>\n<li>My background with web programming and how much I liked it.</li>\n<li>I spent a lot of time thinking about existential questions such as life and death, intelligence and consciousness. I looked to computers with the same intrigue as I looked to people and figure it out that maybe by understanding better computers I could understand better people.</li>\n<li>All my craziest ideas would involve some sort of Artificial Intelligence.</li>\n</ol>\n<p>When the university came, my marks weren’t enough to enter the faculty I have initially chosen, Faculty of Engineering of University of Porto (FEUP). Yet I was able to enter a neighbour school, Institute of Engineering of Porto, where I made the first year. Then I requested transfer to FEUP were I’m making the final four years of the course Master in Informatics and Computing Engineering.</p>\n<p>Meanwhile I’ve been participating with friends, each year, on the Porto Summer Of Code competition, having conquered the second place on the first attempt. I’ve also been participating in other middle year competitions.</p>\n<p>Since the summer of 2015 I’ve also been working with university professors, under three initiation research grants, on a web platform for the support of wind flow modeling experimental campaigns. Here the technologies used were AngularJS and NodeJS based.</p>\n<p>This a bit of who I am. At least so far.</p>\n<p>If you think that you might have common interests with me, don’t hesitate in contacting me.</p>\n<p>Think less, talk less, do more.<br>Daniel</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Hi!,</p>\n<p>My name is Daniel Fernandes Gomes, I was born in 94 and I’m from S. Paio de Oleiros; a northen, near the coast, Portuguese town. Though, many times you may also find me using my alias, Danfergo, which is the agglutination of my full name.</p>\n<p>My memories begin with me launching my little toy cars against the wall and playing with everything else I could find at home. Tying chairs and tables with wool yarn was a trend back then.</p>\n<p>Around eight, I’ve my bike’s tire flattened and from their on that was my main toy. I’ve self-learned how to take it apart, fix it and a few years later reinventing it as a tricycle with a steering wheel from an old car.</p>\n<p>At this age, I was studying at the elementary schools of S. Paio de Oleiros and Santa Maria de Lamas (latest year) while attending “sunday school”, which I kept attending until my 9th grade.</p>\n<p>The next studying cycles happened at Colégio Liceal de Santa Maria de Lamas, where I was doing enough to keep reasonable grades while I would be joining a few clubs, in different years: history, english and arts. A lot more seriously, I’ve joined theatre group where I stayed until the end of the high-school, performing during 5 and half years, around 10 different plays.</p>\n<p>At 15, when the social network Hi5 was at its pick, I’ve found the CSS “codes” and browser source view. I was amazed by It and I’ve embarked on this new learning through hacking path. About a year latter I was publishing tutorials on Youtube and on TOUTLOUD, a forum which I created with the help of a few friends and reached around 2500 users. This motivated me to learn my first programming languages: JavaScript and Php. The forum died when the social network disabled HTML/CSS support.</p>\n<p>Around this age I joined the handball teams, at school and at CDC S. Paio de Oleiros though, for logistic reasons It didn’t last one full year. Next I took on classic guitar lessons at TUNA Musical Group from S. Paio de Oleiros, for two years. This led me to, with friends, create COCKAINE,a rock music band. I left after a few months.</p>\n<p>Since early, I’ve questioned every conception it was purposed to me — that’s stupid, was my usual welcome message. But more than put in question I love to propose, dream about and build my own ideas. Specially for problems that even after long periods of reflection keep staying stupid.</p>\n<p>At my ninth grade I started questioning students association and that led me to, at eleventh, with the help of friends, run for president. Why not?… And we totally lost. But with the lessons learned and contacts established we decided to run once again. We won. I was the president of students association that year; and what a year we: organized two major parties (w/ crowds over 1500 people), a LAN Party sponsored by the town hall and an ISP provider, provided logistic support and were the representatives on the national secondary festival, as well other smaller events.</p>\n<p>From early I wanted to be an engineer and he only options where what branch: mechanical, electrical, computers, etc. Though, three main points quickly help me take that decision:</p>\n<ol>\n<li>My background with web programming and how much I liked it.</li>\n<li>I spent a lot of time thinking about existential questions such as life and death, intelligence and consciousness. I looked to computers with the same intrigue as I looked to people and figure it out that maybe by understanding better computers I could understand better people.</li>\n<li>All my craziest ideas would involve some sort of Artificial Intelligence.</li>\n</ol>\n<p>When the university came, my marks weren’t enough to enter the faculty I have initially chosen, Faculty of Engineering of University of Porto (FEUP). Yet I was able to enter a neighbour school, Institute of Engineering of Porto, where I made the first year. Then I requested transfer to FEUP were I’m making the final four years of the course Master in Informatics and Computing Engineering.</p>\n<p>Meanwhile I’ve been participating with friends, each year, on the Porto Summer Of Code competition, having conquered the second place on the first attempt. I’ve also been participating in other middle year competitions.</p>\n<p>Since the summer of 2015 I’ve also been working with university professors, under three initiation research grants, on a web platform for the support of wind flow modeling experimental campaigns. Here the technologies used were AngularJS and NodeJS based.</p>\n<p>This a bit of who I am. At least so far.</p>\n<p>If you think that you might have common interests with me, don’t hesitate in contacting me.</p>\n<p>Think less, talk less, do more.<br>Daniel</p>\n"}],"Post":[{"title":"GarmNet: Improving Global with Local Perception for Robotic Laundry Folding","author":"danfergo","cover":"cover.png","date":"2019-03-02T22:29:00.000Z","_content":"Developing autonomous assistants to help with domestic tasks is a vital topic in robotics research. Among these tasks, garment folding is one of them that is still far from being achieved mainly due to the large number of possible configurations that a crumpled piece of clothing may exhibit. Research has been done on either estimating the pose of the garment as a whole or detecting the landmarks for grasping separately. However, such works constrain the capability of the robots to perceive the states of the garment by limiting the representations for one single task. In this paper, we propose a novel end-to-end deep learning model named GarmNet that is able to simultaneously localize the garment and detect landmarks for grasping. The localization of the garment represents the global information for recognising the category of the garment, whereas the detection of landmarks can facilitate sub-sequent grasping actions. We train and evaluate our proposed GarmNet model using the CloPeMa Garment dataset that contains 3,330 images of different garment types in different poses. The experiments show that the inclusion of landmark detection (GarmNet-B) can largely improve the garment localization, with an error rate of 24.7% lower. Solutions as ours are important for robotics applications, as these offer scalable to many classes, memory and processing efficient solutions.\n\n\n{% asset_img img_spacial_constraint.png \"Spatial Constraint\" %}\n\n\n### References\n[^1]: GarmNet: Improving Global with Local Perception for Robotic Laundry Folding, TAROS 2019, **[arxiv]([https://arxiv.org/abs/1907.00408])**\n[^2]: Detecting garment and its landmarks MSc dissertation, 2017-07-18. **[UP Open Repository](https://hdl.handle.net/10216/107701)** ","source":"_posts/garmnet.md","raw":"title: 'GarmNet: Improving Global with Local Perception for Robotic Laundry Folding'\nauthor: danfergo\ntags:\n  - featured\n  - vision\ncategories: []\ncover: cover.png\ndate: 2019-03-02 22:29:00\n---\nDeveloping autonomous assistants to help with domestic tasks is a vital topic in robotics research. Among these tasks, garment folding is one of them that is still far from being achieved mainly due to the large number of possible configurations that a crumpled piece of clothing may exhibit. Research has been done on either estimating the pose of the garment as a whole or detecting the landmarks for grasping separately. However, such works constrain the capability of the robots to perceive the states of the garment by limiting the representations for one single task. In this paper, we propose a novel end-to-end deep learning model named GarmNet that is able to simultaneously localize the garment and detect landmarks for grasping. The localization of the garment represents the global information for recognising the category of the garment, whereas the detection of landmarks can facilitate sub-sequent grasping actions. We train and evaluate our proposed GarmNet model using the CloPeMa Garment dataset that contains 3,330 images of different garment types in different poses. The experiments show that the inclusion of landmark detection (GarmNet-B) can largely improve the garment localization, with an error rate of 24.7% lower. Solutions as ours are important for robotics applications, as these offer scalable to many classes, memory and processing efficient solutions.\n\n\n{% asset_img img_spacial_constraint.png \"Spatial Constraint\" %}\n\n\n### References\n[^1]: GarmNet: Improving Global with Local Perception for Robotic Laundry Folding, TAROS 2019, **[arxiv]([https://arxiv.org/abs/1907.00408])**\n[^2]: Detecting garment and its landmarks MSc dissertation, 2017-07-18. **[UP Open Repository](https://hdl.handle.net/10216/107701)** ","slug":"garmnet","published":1,"updated":"2020-05-31T00:20:46.840Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckk4qbnqu0001huvpgr5ubv9w","content":"<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css\"><p>Developing autonomous assistants to help with domestic tasks is a vital topic in robotics research. Among these tasks, garment folding is one of them that is still far from being achieved mainly due to the large number of possible configurations that a crumpled piece of clothing may exhibit. Research has been done on either estimating the pose of the garment as a whole or detecting the landmarks for grasping separately. However, such works constrain the capability of the robots to perceive the states of the garment by limiting the representations for one single task. In this paper, we propose a novel end-to-end deep learning model named GarmNet that is able to simultaneously localize the garment and detect landmarks for grasping. The localization of the garment represents the global information for recognising the category of the garment, whereas the detection of landmarks can facilitate sub-sequent grasping actions. We train and evaluate our proposed GarmNet model using the CloPeMa Garment dataset that contains 3,330 images of different garment types in different poses. The experiments show that the inclusion of landmark detection (GarmNet-B) can largely improve the garment localization, with an error rate of 24.7% lower. Solutions as ours are important for robotics applications, as these offer scalable to many classes, memory and processing efficient solutions.</p>\n<img src=\"/garmnet/img_spacial_constraint.png\" class=\"\" title=\"Spatial Constraint\">\n<h3 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h3><div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style: none; padding-left: 0; margin-left: 40px\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">1.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">GarmNet: Improving Global with Local Perception for Robotic Laundry Folding, TAROS 2019, <strong><a href=\"%5Bhttps://arxiv.org/abs/1907.00408%5D\">arxiv</a></strong><a href=\"#fnref:1\" rev=\"footnote\"> ↩</a></span></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">2.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">Detecting garment and its landmarks MSc dissertation, 2017-07-18. <strong><a href=\"https://hdl.handle.net/10216/107701\">UP Open Repository</a></strong><a href=\"#fnref:2\" rev=\"footnote\"> ↩</a></span></li></ol></div></div>","site":{"data":{}},"excerpt":"","more":"<p>Developing autonomous assistants to help with domestic tasks is a vital topic in robotics research. Among these tasks, garment folding is one of them that is still far from being achieved mainly due to the large number of possible configurations that a crumpled piece of clothing may exhibit. Research has been done on either estimating the pose of the garment as a whole or detecting the landmarks for grasping separately. However, such works constrain the capability of the robots to perceive the states of the garment by limiting the representations for one single task. In this paper, we propose a novel end-to-end deep learning model named GarmNet that is able to simultaneously localize the garment and detect landmarks for grasping. The localization of the garment represents the global information for recognising the category of the garment, whereas the detection of landmarks can facilitate sub-sequent grasping actions. We train and evaluate our proposed GarmNet model using the CloPeMa Garment dataset that contains 3,330 images of different garment types in different poses. The experiments show that the inclusion of landmark detection (GarmNet-B) can largely improve the garment localization, with an error rate of 24.7% lower. Solutions as ours are important for robotics applications, as these offer scalable to many classes, memory and processing efficient solutions.</p>\n<img src=\"/garmnet/img_spacial_constraint.png\" class=\"\" title=\"Spatial Constraint\">\n<h3 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h3><div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style: none; padding-left: 0; margin-left: 40px\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">1.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">GarmNet: Improving Global with Local Perception for Robotic Laundry Folding, TAROS 2019, <strong><a href=\"%5Bhttps://arxiv.org/abs/1907.00408%5D\">arxiv</a></strong><a href=\"#fnref:1\" rev=\"footnote\"> ↩</a></span></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">2.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">Detecting garment and its landmarks MSc dissertation, 2017-07-18. <strong><a href=\"https://hdl.handle.net/10216/107701\">UP Open Repository</a></strong><a href=\"#fnref:2\" rev=\"footnote\"> ↩</a></span></li></ol></div></div>"},{"title":"Clevo P65xRP / Inphtech P600-G on Linux","alias":"2016/10/17/clevo-p65xrp-inphtech-p600-g-on-linux/index.html","author":"danfergo","date":"2016-10-17T00:00:00.000Z","_content":"This is going to be a very atypical post on this blog but, because I spent almost two weeks to figure it out I’ll leave it here. These configs will probably change with time and so, I’ll be updating this post in a logbook manner.\n\n{% asset_img clevo-linux.jpg \"Antergos Gnome on my Clevo P65x\" %}\n\n#### 16th october 2016\n\nI’m running Antergos with Gnome desktop environment and 4.7.6-1-ARCH kernel on UEFI, gpt/lvm partitioning and systemd-boot loader.\n\n1. **Headphone jack:** when I closed the lead, suspended or hibernated the computer the headphone jack would stop working. I found that a lot of people already had this issue and init-headphone package was the solution. You may find it here. https://github.com/Unrud/init-headphone (for arch available at AUR)\n2. **Keyboard backlight:** this was the most obvious and although there was already a driver for clevo/linux, I didn’t found any compatible with the P65x. Fortunately in this repo i was able to find an request/issue suggesting the updates needed to make it compatible with this laptop model. Which worked! A few days latter the developer integrated the changes and now are available by default https://bitbucket.org/lynthium/clevo-xsm-wmi (for arch available at AUR)\n3. **Touchpad:** this was the trickiest one and the main reason why I’ve decided to create this post. After large hours (days) of testing I found this post on askubuntu http://askubuntu.com/questions/525629/touchpad-is-not-recognized which solved the problem. Basically it is related to the i8042 chipset and its (mis)configuration. By passing those flags to the kernel, It will ignore those configs and just turn It on. The Fn+F1 doesn’t work still though; I’ll probably investigate that latter.\n\n\nIf you have any problems/suggestions, please leave comment below.\n\nGood luck.\n\n{% raw %}\n<iframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/236927412&amp;color=000000&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false\"></iframe>\n{% endraw %}","source":"_posts/clevo-p65xrp-inphtech-p600-g-on-linux.md","raw":"title: Clevo P65xRP / Inphtech P600-G on Linux\nalias: 2016/10/17/clevo-p65xrp-inphtech-p600-g-on-linux/index.html\nauthor: danfergo\ntags:\n  - linux\ncategories: []\ndate: 2016-10-17 01:00:00\n---\nThis is going to be a very atypical post on this blog but, because I spent almost two weeks to figure it out I’ll leave it here. These configs will probably change with time and so, I’ll be updating this post in a logbook manner.\n\n{% asset_img clevo-linux.jpg \"Antergos Gnome on my Clevo P65x\" %}\n\n#### 16th october 2016\n\nI’m running Antergos with Gnome desktop environment and 4.7.6-1-ARCH kernel on UEFI, gpt/lvm partitioning and systemd-boot loader.\n\n1. **Headphone jack:** when I closed the lead, suspended or hibernated the computer the headphone jack would stop working. I found that a lot of people already had this issue and init-headphone package was the solution. You may find it here. https://github.com/Unrud/init-headphone (for arch available at AUR)\n2. **Keyboard backlight:** this was the most obvious and although there was already a driver for clevo/linux, I didn’t found any compatible with the P65x. Fortunately in this repo i was able to find an request/issue suggesting the updates needed to make it compatible with this laptop model. Which worked! A few days latter the developer integrated the changes and now are available by default https://bitbucket.org/lynthium/clevo-xsm-wmi (for arch available at AUR)\n3. **Touchpad:** this was the trickiest one and the main reason why I’ve decided to create this post. After large hours (days) of testing I found this post on askubuntu http://askubuntu.com/questions/525629/touchpad-is-not-recognized which solved the problem. Basically it is related to the i8042 chipset and its (mis)configuration. By passing those flags to the kernel, It will ignore those configs and just turn It on. The Fn+F1 doesn’t work still though; I’ll probably investigate that latter.\n\n\nIf you have any problems/suggestions, please leave comment below.\n\nGood luck.\n\n{% raw %}\n<iframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/236927412&amp;color=000000&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false\"></iframe>\n{% endraw %}","slug":"clevo-p65xrp-inphtech-p600-g-on-linux","published":1,"updated":"2020-05-31T00:26:24.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckk4qbnqy0003huvp22tsg6yq","content":"<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css\"><p>This is going to be a very atypical post on this blog but, because I spent almost two weeks to figure it out I’ll leave it here. These configs will probably change with time and so, I’ll be updating this post in a logbook manner.</p>\n<img src=\"/clevo-p65xrp-inphtech-p600-g-on-linux/clevo-linux.jpg\" class=\"\" title=\"Antergos Gnome on my Clevo P65x\">\n<h4 id=\"16th-october-2016\"><a href=\"#16th-october-2016\" class=\"headerlink\" title=\"16th october 2016\"></a>16th october 2016</h4><p>I’m running Antergos with Gnome desktop environment and 4.7.6-1-ARCH kernel on UEFI, gpt/lvm partitioning and systemd-boot loader.</p>\n<ol>\n<li><strong>Headphone jack:</strong> when I closed the lead, suspended or hibernated the computer the headphone jack would stop working. I found that a lot of people already had this issue and init-headphone package was the solution. You may find it here. <a href=\"https://github.com/Unrud/init-headphone\">https://github.com/Unrud/init-headphone</a> (for arch available at AUR)</li>\n<li><strong>Keyboard backlight:</strong> this was the most obvious and although there was already a driver for clevo/linux, I didn’t found any compatible with the P65x. Fortunately in this repo i was able to find an request/issue suggesting the updates needed to make it compatible with this laptop model. Which worked! A few days latter the developer integrated the changes and now are available by default <a href=\"https://bitbucket.org/lynthium/clevo-xsm-wmi\">https://bitbucket.org/lynthium/clevo-xsm-wmi</a> (for arch available at AUR)</li>\n<li><strong>Touchpad:</strong> this was the trickiest one and the main reason why I’ve decided to create this post. After large hours (days) of testing I found this post on askubuntu <a href=\"http://askubuntu.com/questions/525629/touchpad-is-not-recognized\">http://askubuntu.com/questions/525629/touchpad-is-not-recognized</a> which solved the problem. Basically it is related to the i8042 chipset and its (mis)configuration. By passing those flags to the kernel, It will ignore those configs and just turn It on. The Fn+F1 doesn’t work still though; I’ll probably investigate that latter.</li>\n</ol>\n<p>If you have any problems/suggestions, please leave comment below.</p>\n<p>Good luck.</p>\n\n<iframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/236927412&amp;color=000000&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false\"></iframe>\n","site":{"data":{}},"excerpt":"","more":"<p>This is going to be a very atypical post on this blog but, because I spent almost two weeks to figure it out I’ll leave it here. These configs will probably change with time and so, I’ll be updating this post in a logbook manner.</p>\n<img src=\"/clevo-p65xrp-inphtech-p600-g-on-linux/clevo-linux.jpg\" class=\"\" title=\"Antergos Gnome on my Clevo P65x\">\n<h4 id=\"16th-october-2016\"><a href=\"#16th-october-2016\" class=\"headerlink\" title=\"16th october 2016\"></a>16th october 2016</h4><p>I’m running Antergos with Gnome desktop environment and 4.7.6-1-ARCH kernel on UEFI, gpt/lvm partitioning and systemd-boot loader.</p>\n<ol>\n<li><strong>Headphone jack:</strong> when I closed the lead, suspended or hibernated the computer the headphone jack would stop working. I found that a lot of people already had this issue and init-headphone package was the solution. You may find it here. <a href=\"https://github.com/Unrud/init-headphone\">https://github.com/Unrud/init-headphone</a> (for arch available at AUR)</li>\n<li><strong>Keyboard backlight:</strong> this was the most obvious and although there was already a driver for clevo/linux, I didn’t found any compatible with the P65x. Fortunately in this repo i was able to find an request/issue suggesting the updates needed to make it compatible with this laptop model. Which worked! A few days latter the developer integrated the changes and now are available by default <a href=\"https://bitbucket.org/lynthium/clevo-xsm-wmi\">https://bitbucket.org/lynthium/clevo-xsm-wmi</a> (for arch available at AUR)</li>\n<li><strong>Touchpad:</strong> this was the trickiest one and the main reason why I’ve decided to create this post. After large hours (days) of testing I found this post on askubuntu <a href=\"http://askubuntu.com/questions/525629/touchpad-is-not-recognized\">http://askubuntu.com/questions/525629/touchpad-is-not-recognized</a> which solved the problem. Basically it is related to the i8042 chipset and its (mis)configuration. By passing those flags to the kernel, It will ignore those configs and just turn It on. The Fn+F1 doesn’t work still though; I’ll probably investigate that latter.</li>\n</ol>\n<p>If you have any problems/suggestions, please leave comment below.</p>\n<p>Good luck.</p>\n\n<iframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/236927412&amp;color=000000&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false\"></iframe>\n"},{"title":"GelSight Simulation for Sim2Real Learning","author":"danfergo","cover":"cover.png","date":"2021-05-28T22:55:00.000Z","_content":"{% youtube tr6orOcGic0 %}\n\n**This work has been published at RA-L and ICRA 2021**[^1].\n\n\n## Abstract\nMost current works in *Sim2Real* learning for robotic manipulation tasks leverage camera vision that may be significantly occluded by robot hands during the manipulation. Tactile sensing offers complementary information to vision and can compensate for the information loss caused by the occlusion. However, the use of tactile sensing is restricted in the *Sim2Real* research due to no simulated tactile sensors being available. To mitigate the gap, we introduce a novel approach for simulating a GelSight tactile sensor in the commonly used Gazebo simulator. Similar to the real GelSight sensor, the simulated sensor can produce high-resolution images by an optical sensor from the interaction between the touched object and an opaque soft membrane. It can indirectly sense forces, geometry, texture and other properties of the object and enables *Sim2Real* learning with tactile sensing. Preliminary experimental results have shown that the simulated sensor could generate realistic outputs similar to the ones captured by a real GelSight sensor. \n\n{% raw %}\n<img width=\"100%\" src=\"samples_qualitative.jpg\" title=\"Data collection using an FDM 3D Printer\" margin:3rem auto 1rem;\"> \n{% endraw %}\n\n**Figure 1**. Samples collected using a GelSight 2014 sensor (top row) and the corresponding simulations: using[^2] (2nd row), the ***Single Gaussian*** (3rd row) and the ***Difference of Gaussians*** (4th row) for the elastomer heightmap approximation, for a GelSight 2017 sensor (last row). As seen in the listed tactile images, the generated samples look realistic and quite similar to the real ones, being able to replicate internal light configurations of different sensors. \n\n\n## Materials\nIn the table bellow, the necessary materials for reproducing this work are provided. These include the STL files for printing the 21 set of objects and support mount, the raw real and virtual datasets, and the aligned datasets using the per-object alignment method. Please refer to the paper for more details about the experiments.\n\n| DESCRIPTION           |  FILE    |\n|-----------------------|:--------:|\n| **Source-code**: *ROS packages + Experiments scripts* | ** [GitHub](https://github.com/danfergo/gelsight_simulation) **  |\n| **Unaligned data**: *real RGB and virtual depth maps* | ** [unaligned.zip](https://mega.nz/file/xKxRxC6L#D7QYagQzDjHWKdqzAO46lNFiW2S_wpJu6Y1HLO9hJjE) **  |\n| **Aligned data**: *globally, real RGB and virtual depth maps + RGB* | ** [aligned_g.zip](https://mega.nz/file/ZbZAxRZL#RzB4zxJoYnAlgC1WEmRMWv1Y67drW3bbFPA1PmMFCt8) ** |\n| **Aligned data**: *per-object, real RGB and virtual depth maps + RGB* | ** [aligned_po.zip](https://mega.nz/file/QbYUyBpB#tE3GXRrbl1wh8Pd-kw0ib4SzoIOfmLYEwo_2I_BZpRg) ** |\n| **Texture maps**: used to augment the training data, for  *Sim2Real* TL | **[textures.zip](https://mega.nz/file/xL5jyKYa#leghrMB-qdUaLYHvtlsAo-4v4PEmslPmMblmsabxj5s)** |\n| **3D printable STL & CAD files**: *object set used in the experiments* | ** [object_set.zip](https://mega.nz/file/VewxyQTD#AppWhGiuUFy4bBeIekexonlm-DyQ7MoP9VMri3sy4U8)  ** |\n\n\n## ViTac workshop \n\nThis GelSight Simulation method was firstly proposed at the [2019 ICRA ViTac Worshop](http://wordpress.csc.liv.ac.uk/smartlab/icra-2019-vitac-workshop/) and, given the  [interest shown](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1347008206220158376) by the community, now revised and extended in a new publication[^1]. For instance, the initial elastomer deformation approximation approach, generated unrealistic sharp contouring around the in-contact areas, as shown in **Figure 2**. Improvements achieved now, are shown in **Figure 3.**\n\n{% asset_img vitac2019workshop.jpg %}\n\n**Figure 2**. Real and synthetic tactile samples next to the corresponding experimental setup, as in the first work[^1]. experimental setup. Samples were collected using ordinary objects, and the experimental setup consisted on a GelSight 2014 installed on a UR5 robotic arm.\n\n{% asset_img heightmap.jpg %}\n\n**Figure 3**. Comparison of different methods for approximating elastomer deformations: without any smoothing effects (***Before Smoothing***), smoothed with a single Gaussian filter (***Single Gaussian***) and smoothed with the DoG (***Difference of Gaussians***).  \n\n## Experimental Setup \nTo produce the necessary real datasets, a GelSight sensor is mounted onto a Fused Deposition Modeling (FDM) 3D printer A30 from Geeetech. A set of objects with different shapes on the top is designed and 3D printed using the Form 2 Stereolithography (SLA) 3D printer. A Virtual World comprised of a FDM printer, a GelSight sensor and a set of virtual objects, is also set up. Identical real and virtual datasets are then collected.\n\n{% raw %}\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"data_collection.webm\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n{% endraw %}\n\n{% raw %}\n<img width=\"100%\" src=\"object_set.jpg\" title=\"Data collection using an FDM 3D Printer\" style=\"max-width:50rem\"> \n{% endraw %}\n\n**Figure 5. ** The objects set: Hexagon, Dot-in, Moon, Large Sphere, Pacman, Flat Slab, Wave, Cylinder, Triangle, Random Prism, Line, Torus, Curved Surface, Dots, Cone, Small Sphere, Rectangular Prism, Side Cylinder, Open Shell, Parallel lines and Crossed Lines.\n\n## Sim2Real transfer learning\n\nOne aspect to consider in the *Sim2Real* learning is the *Sim2Real* gap that results from characteristics of the real world being not modeled in the simulation. In our case, we find that one major difference between the real and synthetic samples are the textures introduced by the 3D printing process. To mitigate this issue, we create twelve texture maps using GIMP that resemble the textures observed in the real samples, as shown in **Figure 1**. By randomly perturbing the captured virtual depth-maps with such textures, we are able to produce an effective data augmentation scheme that significantly improves the *Sim2Real* transition, from a 43.76% classification accuracy to 76.19%, in the real data.\n\n{% asset_img texture_augmented.jpg %}\n\n\n\n\n** Figure 4 ** On the top row, four of the twelve textures created to perturb the captured virtual depth-maps, to address the *Sim2Real* gap.  On the bottom row, corresponding augmented samples fed to the Neural Network during training, after perturbing the depth-map  with the randomly distorted texture, generating the RGB tactile sample using the proposed method, and applying a random augmentation transformation.\n\n\n### References\n[^1]: Daniel Fernandes Gomes, Paolo Paoletti and Shan Luo, &#65282;Generation of GelSight Tactile Images for Sim2Real Learning&#65282;,  **[RA Letters](https://ieeexplore.ieee.org/abstract/document/9369877)**, **[ArXiv preprint](https://arxiv.org/abs/2101.07169)**\n[^2]: Daniel Fernandes Gomes, Achu Wilson and Shan Luo, &#65282;GelSight Simulation for Sim2Real Learning&#65282;, ViTac Workshop ICRA 2019. **[paper](http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2019/06/ICRA2019ViTac_paper_8.pdf)**","source":"_posts/gelsight-simulation.md","raw":"title: GelSight Simulation for Sim2Real Learning\nauthor: danfergo\ntags:\n  - featured\n  - touch\ncategories: []\ncover: cover.png\ndate: 2021-05-28 23:55:00\n---\n{% youtube tr6orOcGic0 %}\n\n**This work has been published at RA-L and ICRA 2021**[^1].\n\n\n## Abstract\nMost current works in *Sim2Real* learning for robotic manipulation tasks leverage camera vision that may be significantly occluded by robot hands during the manipulation. Tactile sensing offers complementary information to vision and can compensate for the information loss caused by the occlusion. However, the use of tactile sensing is restricted in the *Sim2Real* research due to no simulated tactile sensors being available. To mitigate the gap, we introduce a novel approach for simulating a GelSight tactile sensor in the commonly used Gazebo simulator. Similar to the real GelSight sensor, the simulated sensor can produce high-resolution images by an optical sensor from the interaction between the touched object and an opaque soft membrane. It can indirectly sense forces, geometry, texture and other properties of the object and enables *Sim2Real* learning with tactile sensing. Preliminary experimental results have shown that the simulated sensor could generate realistic outputs similar to the ones captured by a real GelSight sensor. \n\n{% raw %}\n<img width=\"100%\" src=\"samples_qualitative.jpg\" title=\"Data collection using an FDM 3D Printer\" margin:3rem auto 1rem;\"> \n{% endraw %}\n\n**Figure 1**. Samples collected using a GelSight 2014 sensor (top row) and the corresponding simulations: using[^2] (2nd row), the ***Single Gaussian*** (3rd row) and the ***Difference of Gaussians*** (4th row) for the elastomer heightmap approximation, for a GelSight 2017 sensor (last row). As seen in the listed tactile images, the generated samples look realistic and quite similar to the real ones, being able to replicate internal light configurations of different sensors. \n\n\n## Materials\nIn the table bellow, the necessary materials for reproducing this work are provided. These include the STL files for printing the 21 set of objects and support mount, the raw real and virtual datasets, and the aligned datasets using the per-object alignment method. Please refer to the paper for more details about the experiments.\n\n| DESCRIPTION           |  FILE    |\n|-----------------------|:--------:|\n| **Source-code**: *ROS packages + Experiments scripts* | ** [GitHub](https://github.com/danfergo/gelsight_simulation) **  |\n| **Unaligned data**: *real RGB and virtual depth maps* | ** [unaligned.zip](https://mega.nz/file/xKxRxC6L#D7QYagQzDjHWKdqzAO46lNFiW2S_wpJu6Y1HLO9hJjE) **  |\n| **Aligned data**: *globally, real RGB and virtual depth maps + RGB* | ** [aligned_g.zip](https://mega.nz/file/ZbZAxRZL#RzB4zxJoYnAlgC1WEmRMWv1Y67drW3bbFPA1PmMFCt8) ** |\n| **Aligned data**: *per-object, real RGB and virtual depth maps + RGB* | ** [aligned_po.zip](https://mega.nz/file/QbYUyBpB#tE3GXRrbl1wh8Pd-kw0ib4SzoIOfmLYEwo_2I_BZpRg) ** |\n| **Texture maps**: used to augment the training data, for  *Sim2Real* TL | **[textures.zip](https://mega.nz/file/xL5jyKYa#leghrMB-qdUaLYHvtlsAo-4v4PEmslPmMblmsabxj5s)** |\n| **3D printable STL & CAD files**: *object set used in the experiments* | ** [object_set.zip](https://mega.nz/file/VewxyQTD#AppWhGiuUFy4bBeIekexonlm-DyQ7MoP9VMri3sy4U8)  ** |\n\n\n## ViTac workshop \n\nThis GelSight Simulation method was firstly proposed at the [2019 ICRA ViTac Worshop](http://wordpress.csc.liv.ac.uk/smartlab/icra-2019-vitac-workshop/) and, given the  [interest shown](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1347008206220158376) by the community, now revised and extended in a new publication[^1]. For instance, the initial elastomer deformation approximation approach, generated unrealistic sharp contouring around the in-contact areas, as shown in **Figure 2**. Improvements achieved now, are shown in **Figure 3.**\n\n{% asset_img vitac2019workshop.jpg %}\n\n**Figure 2**. Real and synthetic tactile samples next to the corresponding experimental setup, as in the first work[^1]. experimental setup. Samples were collected using ordinary objects, and the experimental setup consisted on a GelSight 2014 installed on a UR5 robotic arm.\n\n{% asset_img heightmap.jpg %}\n\n**Figure 3**. Comparison of different methods for approximating elastomer deformations: without any smoothing effects (***Before Smoothing***), smoothed with a single Gaussian filter (***Single Gaussian***) and smoothed with the DoG (***Difference of Gaussians***).  \n\n## Experimental Setup \nTo produce the necessary real datasets, a GelSight sensor is mounted onto a Fused Deposition Modeling (FDM) 3D printer A30 from Geeetech. A set of objects with different shapes on the top is designed and 3D printed using the Form 2 Stereolithography (SLA) 3D printer. A Virtual World comprised of a FDM printer, a GelSight sensor and a set of virtual objects, is also set up. Identical real and virtual datasets are then collected.\n\n{% raw %}\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"data_collection.webm\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n{% endraw %}\n\n{% raw %}\n<img width=\"100%\" src=\"object_set.jpg\" title=\"Data collection using an FDM 3D Printer\" style=\"max-width:50rem\"> \n{% endraw %}\n\n**Figure 5. ** The objects set: Hexagon, Dot-in, Moon, Large Sphere, Pacman, Flat Slab, Wave, Cylinder, Triangle, Random Prism, Line, Torus, Curved Surface, Dots, Cone, Small Sphere, Rectangular Prism, Side Cylinder, Open Shell, Parallel lines and Crossed Lines.\n\n## Sim2Real transfer learning\n\nOne aspect to consider in the *Sim2Real* learning is the *Sim2Real* gap that results from characteristics of the real world being not modeled in the simulation. In our case, we find that one major difference between the real and synthetic samples are the textures introduced by the 3D printing process. To mitigate this issue, we create twelve texture maps using GIMP that resemble the textures observed in the real samples, as shown in **Figure 1**. By randomly perturbing the captured virtual depth-maps with such textures, we are able to produce an effective data augmentation scheme that significantly improves the *Sim2Real* transition, from a 43.76% classification accuracy to 76.19%, in the real data.\n\n{% asset_img texture_augmented.jpg %}\n\n\n\n\n** Figure 4 ** On the top row, four of the twelve textures created to perturb the captured virtual depth-maps, to address the *Sim2Real* gap.  On the bottom row, corresponding augmented samples fed to the Neural Network during training, after perturbing the depth-map  with the randomly distorted texture, generating the RGB tactile sample using the proposed method, and applying a random augmentation transformation.\n\n\n### References\n[^1]: Daniel Fernandes Gomes, Paolo Paoletti and Shan Luo, &#65282;Generation of GelSight Tactile Images for Sim2Real Learning&#65282;,  **[RA Letters](https://ieeexplore.ieee.org/abstract/document/9369877)**, **[ArXiv preprint](https://arxiv.org/abs/2101.07169)**\n[^2]: Daniel Fernandes Gomes, Achu Wilson and Shan Luo, &#65282;GelSight Simulation for Sim2Real Learning&#65282;, ViTac Workshop ICRA 2019. **[paper](http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2019/06/ICRA2019ViTac_paper_8.pdf)**","slug":"gelsight-simulation","published":1,"updated":"2021-05-28T19:04:39.660Z","_id":"ckk4qbnr20006huvphnvcgdiv","comments":1,"layout":"post","photos":[],"link":"","content":"<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css\"><div class=\"video-container\"><iframe src=\"https://www.youtube.com/embed/tr6orOcGic0\" frameborder=\"0\" loading=\"lazy\" allowfullscreen></iframe></div>\n<p><strong>This work has been published at RA-L and ICRA 2021</strong><sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"Daniel Fernandes Gomes, Paolo Paoletti and Shan Luo, &#65282;Generation of GelSight Tactile Images for Sim2Real Learning&#65282;,  **[RA Letters](https://ieeexplore.ieee.org/abstract/document/9369877)**, **[ArXiv preprint](https://arxiv.org/abs/2101.07169)**\n\">[1]</span></a></sup>.</p>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Most current works in <em>Sim2Real</em> learning for robotic manipulation tasks leverage camera vision that may be significantly occluded by robot hands during the manipulation. Tactile sensing offers complementary information to vision and can compensate for the information loss caused by the occlusion. However, the use of tactile sensing is restricted in the <em>Sim2Real</em> research due to no simulated tactile sensors being available. To mitigate the gap, we introduce a novel approach for simulating a GelSight tactile sensor in the commonly used Gazebo simulator. Similar to the real GelSight sensor, the simulated sensor can produce high-resolution images by an optical sensor from the interaction between the touched object and an opaque soft membrane. It can indirectly sense forces, geometry, texture and other properties of the object and enables <em>Sim2Real</em> learning with tactile sensing. Preliminary experimental results have shown that the simulated sensor could generate realistic outputs similar to the ones captured by a real GelSight sensor. </p>\n\n<img width=\"100%\" src=\"samples_qualitative.jpg\" title=\"Data collection using an FDM 3D Printer\" margin:3rem auto 1rem;\"> \n\n<p><strong>Figure 1</strong>. Samples collected using a GelSight 2014 sensor (top row) and the corresponding simulations: using<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"Daniel Fernandes Gomes, Achu Wilson and Shan Luo, &#65282;GelSight Simulation for Sim2Real Learning&#65282;, ViTac Workshop ICRA 2019. **[paper](http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2019/06/ICRA2019ViTac_paper_8.pdf)**\">[2]</span></a></sup> (2nd row), the <strong><em>Single Gaussian</em></strong> (3rd row) and the <strong><em>Difference of Gaussians</em></strong> (4th row) for the elastomer heightmap approximation, for a GelSight 2017 sensor (last row). As seen in the listed tactile images, the generated samples look realistic and quite similar to the real ones, being able to replicate internal light configurations of different sensors. </p>\n<h2 id=\"Materials\"><a href=\"#Materials\" class=\"headerlink\" title=\"Materials\"></a>Materials</h2><p>In the table bellow, the necessary materials for reproducing this work are provided. These include the STL files for printing the 21 set of objects and support mount, the raw real and virtual datasets, and the aligned datasets using the per-object alignment method. Please refer to the paper for more details about the experiments.</p>\n<table>\n<thead>\n<tr>\n<th>DESCRIPTION</th>\n<th style=\"text-align:center\">FILE</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Source-code</strong>: <em>ROS packages + Experiments scripts</em></td>\n<td style=\"text-align:center\"><strong> <a href=\"https://github.com/danfergo/gelsight_simulation\">GitHub</a> </strong></td>\n</tr>\n<tr>\n<td><strong>Unaligned data</strong>: <em>real RGB and virtual depth maps</em></td>\n<td style=\"text-align:center\"><strong> <a href=\"https://mega.nz/file/xKxRxC6L#D7QYagQzDjHWKdqzAO46lNFiW2S_wpJu6Y1HLO9hJjE\">unaligned.zip</a> </strong></td>\n</tr>\n<tr>\n<td><strong>Aligned data</strong>: <em>globally, real RGB and virtual depth maps + RGB</em></td>\n<td style=\"text-align:center\"><strong> <a href=\"https://mega.nz/file/ZbZAxRZL#RzB4zxJoYnAlgC1WEmRMWv1Y67drW3bbFPA1PmMFCt8\">aligned_g.zip</a> </strong></td>\n</tr>\n<tr>\n<td><strong>Aligned data</strong>: <em>per-object, real RGB and virtual depth maps + RGB</em></td>\n<td style=\"text-align:center\"><strong> <a href=\"https://mega.nz/file/QbYUyBpB#tE3GXRrbl1wh8Pd-kw0ib4SzoIOfmLYEwo_2I_BZpRg\">aligned_po.zip</a> </strong></td>\n</tr>\n<tr>\n<td><strong>Texture maps</strong>: used to augment the training data, for  <em>Sim2Real</em> TL</td>\n<td style=\"text-align:center\"><strong><a href=\"https://mega.nz/file/xL5jyKYa#leghrMB-qdUaLYHvtlsAo-4v4PEmslPmMblmsabxj5s\">textures.zip</a></strong></td>\n</tr>\n<tr>\n<td><strong>3D printable STL &amp; CAD files</strong>: <em>object set used in the experiments</em></td>\n<td style=\"text-align:center\"><strong> <a href=\"https://mega.nz/file/VewxyQTD#AppWhGiuUFy4bBeIekexonlm-DyQ7MoP9VMri3sy4U8\">object_set.zip</a>  </strong></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"ViTac-workshop\"><a href=\"#ViTac-workshop\" class=\"headerlink\" title=\"ViTac workshop\"></a>ViTac workshop</h2><p>This GelSight Simulation method was firstly proposed at the <a href=\"http://wordpress.csc.liv.ac.uk/smartlab/icra-2019-vitac-workshop/\">2019 ICRA ViTac Worshop</a> and, given the  <a href=\"https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cites=1347008206220158376\">interest shown</a> by the community, now revised and extended in a new publication<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"Daniel Fernandes Gomes, Paolo Paoletti and Shan Luo, &#65282;Generation of GelSight Tactile Images for Sim2Real Learning&#65282;,  **[RA Letters](https://ieeexplore.ieee.org/abstract/document/9369877)**, **[ArXiv preprint](https://arxiv.org/abs/2101.07169)**\n\">[1]</span></a></sup>. For instance, the initial elastomer deformation approximation approach, generated unrealistic sharp contouring around the in-contact areas, as shown in <strong>Figure 2</strong>. Improvements achieved now, are shown in <strong>Figure 3.</strong></p>\n<img src=\"/gelsight-simulation/vitac2019workshop.jpg\" class=\"\">\n<p><strong>Figure 2</strong>. Real and synthetic tactile samples next to the corresponding experimental setup, as in the first work<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"Daniel Fernandes Gomes, Paolo Paoletti and Shan Luo, &#65282;Generation of GelSight Tactile Images for Sim2Real Learning&#65282;,  **[RA Letters](https://ieeexplore.ieee.org/abstract/document/9369877)**, **[ArXiv preprint](https://arxiv.org/abs/2101.07169)**\n\">[1]</span></a></sup>. experimental setup. Samples were collected using ordinary objects, and the experimental setup consisted on a GelSight 2014 installed on a UR5 robotic arm.</p>\n<img src=\"/gelsight-simulation/heightmap.jpg\" class=\"\">\n<p><strong>Figure 3</strong>. Comparison of different methods for approximating elastomer deformations: without any smoothing effects (<strong><em>Before Smoothing</em></strong>), smoothed with a single Gaussian filter (<strong><em>Single Gaussian</em></strong>) and smoothed with the DoG (<strong><em>Difference of Gaussians</em></strong>).  </p>\n<h2 id=\"Experimental-Setup\"><a href=\"#Experimental-Setup\" class=\"headerlink\" title=\"Experimental Setup\"></a>Experimental Setup</h2><p>To produce the necessary real datasets, a GelSight sensor is mounted onto a Fused Deposition Modeling (FDM) 3D printer A30 from Geeetech. A set of objects with different shapes on the top is designed and 3D printed using the Form 2 Stereolithography (SLA) 3D printer. A Virtual World comprised of a FDM printer, a GelSight sensor and a set of virtual objects, is also set up. Identical real and virtual datasets are then collected.</p>\n\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"data_collection.webm\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n\n\n<img width=\"100%\" src=\"object_set.jpg\" title=\"Data collection using an FDM 3D Printer\" style=\"max-width:50rem\"> \n\n<p><strong>Figure 5. </strong> The objects set: Hexagon, Dot-in, Moon, Large Sphere, Pacman, Flat Slab, Wave, Cylinder, Triangle, Random Prism, Line, Torus, Curved Surface, Dots, Cone, Small Sphere, Rectangular Prism, Side Cylinder, Open Shell, Parallel lines and Crossed Lines.</p>\n<h2 id=\"Sim2Real-transfer-learning\"><a href=\"#Sim2Real-transfer-learning\" class=\"headerlink\" title=\"Sim2Real transfer learning\"></a>Sim2Real transfer learning</h2><p>One aspect to consider in the <em>Sim2Real</em> learning is the <em>Sim2Real</em> gap that results from characteristics of the real world being not modeled in the simulation. In our case, we find that one major difference between the real and synthetic samples are the textures introduced by the 3D printing process. To mitigate this issue, we create twelve texture maps using GIMP that resemble the textures observed in the real samples, as shown in <strong>Figure 1</strong>. By randomly perturbing the captured virtual depth-maps with such textures, we are able to produce an effective data augmentation scheme that significantly improves the <em>Sim2Real</em> transition, from a 43.76% classification accuracy to 76.19%, in the real data.</p>\n<img src=\"/gelsight-simulation/texture_augmented.jpg\" class=\"\">\n<p><strong> Figure 4 </strong> On the top row, four of the twelve textures created to perturb the captured virtual depth-maps, to address the <em>Sim2Real</em> gap.  On the bottom row, corresponding augmented samples fed to the Neural Network during training, after perturbing the depth-map  with the randomly distorted texture, generating the RGB tactile sample using the proposed method, and applying a random augmentation transformation.</p>\n<h3 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h3><div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style: none; padding-left: 0; margin-left: 40px\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">1.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">Daniel Fernandes Gomes, Paolo Paoletti and Shan Luo, ＂Generation of GelSight Tactile Images for Sim2Real Learning＂,  <strong><a href=\"https://ieeexplore.ieee.org/abstract/document/9369877\">RA Letters</a></strong>, <strong><a href=\"https://arxiv.org/abs/2101.07169\">ArXiv preprint</a></strong><a href=\"#fnref:1\" rev=\"footnote\"> ↩</a></span></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">2.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">Daniel Fernandes Gomes, Achu Wilson and Shan Luo, ＂GelSight Simulation for Sim2Real Learning＂, ViTac Workshop ICRA 2019. <strong><a href=\"http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2019/06/ICRA2019ViTac_paper_8.pdf\">paper</a></strong><a href=\"#fnref:2\" rev=\"footnote\"> ↩</a></span></li></ol></div></div>","site":{"data":{}},"excerpt":"","more":"<div class=\"video-container\"><iframe src=\"https://www.youtube.com/embed/tr6orOcGic0\" frameborder=\"0\" loading=\"lazy\" allowfullscreen></iframe></div>\n<p><strong>This work has been published at RA-L and ICRA 2021</strong><sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"Daniel Fernandes Gomes, Paolo Paoletti and Shan Luo, &#65282;Generation of GelSight Tactile Images for Sim2Real Learning&#65282;,  **[RA Letters](https://ieeexplore.ieee.org/abstract/document/9369877)**, **[ArXiv preprint](https://arxiv.org/abs/2101.07169)**\n\">[1]</span></a></sup>.</p>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Most current works in <em>Sim2Real</em> learning for robotic manipulation tasks leverage camera vision that may be significantly occluded by robot hands during the manipulation. Tactile sensing offers complementary information to vision and can compensate for the information loss caused by the occlusion. However, the use of tactile sensing is restricted in the <em>Sim2Real</em> research due to no simulated tactile sensors being available. To mitigate the gap, we introduce a novel approach for simulating a GelSight tactile sensor in the commonly used Gazebo simulator. Similar to the real GelSight sensor, the simulated sensor can produce high-resolution images by an optical sensor from the interaction between the touched object and an opaque soft membrane. It can indirectly sense forces, geometry, texture and other properties of the object and enables <em>Sim2Real</em> learning with tactile sensing. Preliminary experimental results have shown that the simulated sensor could generate realistic outputs similar to the ones captured by a real GelSight sensor. </p>\n\n<img width=\"100%\" src=\"samples_qualitative.jpg\" title=\"Data collection using an FDM 3D Printer\" margin:3rem auto 1rem;\"> \n\n<p><strong>Figure 1</strong>. Samples collected using a GelSight 2014 sensor (top row) and the corresponding simulations: using<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"Daniel Fernandes Gomes, Achu Wilson and Shan Luo, &#65282;GelSight Simulation for Sim2Real Learning&#65282;, ViTac Workshop ICRA 2019. **[paper](http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2019/06/ICRA2019ViTac_paper_8.pdf)**\">[2]</span></a></sup> (2nd row), the <strong><em>Single Gaussian</em></strong> (3rd row) and the <strong><em>Difference of Gaussians</em></strong> (4th row) for the elastomer heightmap approximation, for a GelSight 2017 sensor (last row). As seen in the listed tactile images, the generated samples look realistic and quite similar to the real ones, being able to replicate internal light configurations of different sensors. </p>\n<h2 id=\"Materials\"><a href=\"#Materials\" class=\"headerlink\" title=\"Materials\"></a>Materials</h2><p>In the table bellow, the necessary materials for reproducing this work are provided. These include the STL files for printing the 21 set of objects and support mount, the raw real and virtual datasets, and the aligned datasets using the per-object alignment method. Please refer to the paper for more details about the experiments.</p>\n<table>\n<thead>\n<tr>\n<th>DESCRIPTION</th>\n<th style=\"text-align:center\">FILE</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Source-code</strong>: <em>ROS packages + Experiments scripts</em></td>\n<td style=\"text-align:center\"><strong> <a href=\"https://github.com/danfergo/gelsight_simulation\">GitHub</a> </strong></td>\n</tr>\n<tr>\n<td><strong>Unaligned data</strong>: <em>real RGB and virtual depth maps</em></td>\n<td style=\"text-align:center\"><strong> <a href=\"https://mega.nz/file/xKxRxC6L#D7QYagQzDjHWKdqzAO46lNFiW2S_wpJu6Y1HLO9hJjE\">unaligned.zip</a> </strong></td>\n</tr>\n<tr>\n<td><strong>Aligned data</strong>: <em>globally, real RGB and virtual depth maps + RGB</em></td>\n<td style=\"text-align:center\"><strong> <a href=\"https://mega.nz/file/ZbZAxRZL#RzB4zxJoYnAlgC1WEmRMWv1Y67drW3bbFPA1PmMFCt8\">aligned_g.zip</a> </strong></td>\n</tr>\n<tr>\n<td><strong>Aligned data</strong>: <em>per-object, real RGB and virtual depth maps + RGB</em></td>\n<td style=\"text-align:center\"><strong> <a href=\"https://mega.nz/file/QbYUyBpB#tE3GXRrbl1wh8Pd-kw0ib4SzoIOfmLYEwo_2I_BZpRg\">aligned_po.zip</a> </strong></td>\n</tr>\n<tr>\n<td><strong>Texture maps</strong>: used to augment the training data, for  <em>Sim2Real</em> TL</td>\n<td style=\"text-align:center\"><strong><a href=\"https://mega.nz/file/xL5jyKYa#leghrMB-qdUaLYHvtlsAo-4v4PEmslPmMblmsabxj5s\">textures.zip</a></strong></td>\n</tr>\n<tr>\n<td><strong>3D printable STL &amp; CAD files</strong>: <em>object set used in the experiments</em></td>\n<td style=\"text-align:center\"><strong> <a href=\"https://mega.nz/file/VewxyQTD#AppWhGiuUFy4bBeIekexonlm-DyQ7MoP9VMri3sy4U8\">object_set.zip</a>  </strong></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"ViTac-workshop\"><a href=\"#ViTac-workshop\" class=\"headerlink\" title=\"ViTac workshop\"></a>ViTac workshop</h2><p>This GelSight Simulation method was firstly proposed at the <a href=\"http://wordpress.csc.liv.ac.uk/smartlab/icra-2019-vitac-workshop/\">2019 ICRA ViTac Worshop</a> and, given the  <a href=\"https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cites=1347008206220158376\">interest shown</a> by the community, now revised and extended in a new publication<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"Daniel Fernandes Gomes, Paolo Paoletti and Shan Luo, &#65282;Generation of GelSight Tactile Images for Sim2Real Learning&#65282;,  **[RA Letters](https://ieeexplore.ieee.org/abstract/document/9369877)**, **[ArXiv preprint](https://arxiv.org/abs/2101.07169)**\n\">[1]</span></a></sup>. For instance, the initial elastomer deformation approximation approach, generated unrealistic sharp contouring around the in-contact areas, as shown in <strong>Figure 2</strong>. Improvements achieved now, are shown in <strong>Figure 3.</strong></p>\n<img src=\"/gelsight-simulation/vitac2019workshop.jpg\" class=\"\">\n<p><strong>Figure 2</strong>. Real and synthetic tactile samples next to the corresponding experimental setup, as in the first work<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"Daniel Fernandes Gomes, Paolo Paoletti and Shan Luo, &#65282;Generation of GelSight Tactile Images for Sim2Real Learning&#65282;,  **[RA Letters](https://ieeexplore.ieee.org/abstract/document/9369877)**, **[ArXiv preprint](https://arxiv.org/abs/2101.07169)**\n\">[1]</span></a></sup>. experimental setup. Samples were collected using ordinary objects, and the experimental setup consisted on a GelSight 2014 installed on a UR5 robotic arm.</p>\n<img src=\"/gelsight-simulation/heightmap.jpg\" class=\"\">\n<p><strong>Figure 3</strong>. Comparison of different methods for approximating elastomer deformations: without any smoothing effects (<strong><em>Before Smoothing</em></strong>), smoothed with a single Gaussian filter (<strong><em>Single Gaussian</em></strong>) and smoothed with the DoG (<strong><em>Difference of Gaussians</em></strong>).  </p>\n<h2 id=\"Experimental-Setup\"><a href=\"#Experimental-Setup\" class=\"headerlink\" title=\"Experimental Setup\"></a>Experimental Setup</h2><p>To produce the necessary real datasets, a GelSight sensor is mounted onto a Fused Deposition Modeling (FDM) 3D printer A30 from Geeetech. A set of objects with different shapes on the top is designed and 3D printed using the Form 2 Stereolithography (SLA) 3D printer. A Virtual World comprised of a FDM printer, a GelSight sensor and a set of virtual objects, is also set up. Identical real and virtual datasets are then collected.</p>\n\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"data_collection.webm\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n\n\n<img width=\"100%\" src=\"object_set.jpg\" title=\"Data collection using an FDM 3D Printer\" style=\"max-width:50rem\"> \n\n<p><strong>Figure 5. </strong> The objects set: Hexagon, Dot-in, Moon, Large Sphere, Pacman, Flat Slab, Wave, Cylinder, Triangle, Random Prism, Line, Torus, Curved Surface, Dots, Cone, Small Sphere, Rectangular Prism, Side Cylinder, Open Shell, Parallel lines and Crossed Lines.</p>\n<h2 id=\"Sim2Real-transfer-learning\"><a href=\"#Sim2Real-transfer-learning\" class=\"headerlink\" title=\"Sim2Real transfer learning\"></a>Sim2Real transfer learning</h2><p>One aspect to consider in the <em>Sim2Real</em> learning is the <em>Sim2Real</em> gap that results from characteristics of the real world being not modeled in the simulation. In our case, we find that one major difference between the real and synthetic samples are the textures introduced by the 3D printing process. To mitigate this issue, we create twelve texture maps using GIMP that resemble the textures observed in the real samples, as shown in <strong>Figure 1</strong>. By randomly perturbing the captured virtual depth-maps with such textures, we are able to produce an effective data augmentation scheme that significantly improves the <em>Sim2Real</em> transition, from a 43.76% classification accuracy to 76.19%, in the real data.</p>\n<img src=\"/gelsight-simulation/texture_augmented.jpg\" class=\"\">\n<p><strong> Figure 4 </strong> On the top row, four of the twelve textures created to perturb the captured virtual depth-maps, to address the <em>Sim2Real</em> gap.  On the bottom row, corresponding augmented samples fed to the Neural Network during training, after perturbing the depth-map  with the randomly distorted texture, generating the RGB tactile sample using the proposed method, and applying a random augmentation transformation.</p>\n<h3 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h3><div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style: none; padding-left: 0; margin-left: 40px\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">1.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">Daniel Fernandes Gomes, Paolo Paoletti and Shan Luo, ＂Generation of GelSight Tactile Images for Sim2Real Learning＂,  <strong><a href=\"https://ieeexplore.ieee.org/abstract/document/9369877\">RA Letters</a></strong>, <strong><a href=\"https://arxiv.org/abs/2101.07169\">ArXiv preprint</a></strong><a href=\"#fnref:1\" rev=\"footnote\"> ↩</a></span></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">2.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">Daniel Fernandes Gomes, Achu Wilson and Shan Luo, ＂GelSight Simulation for Sim2Real Learning＂, ViTac Workshop ICRA 2019. <strong><a href=\"http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2019/06/ICRA2019ViTac_paper_8.pdf\">paper</a></strong><a href=\"#fnref:2\" rev=\"footnote\"> ↩</a></span></li></ol></div></div>"},{"title":"GelTip: A Finger-shaped Optical Tactile Sensor for Robotic Manipulation","author":"danfergo","cover":"cover.jpg","date":"2020-03-07T20:08:00.000Z","_content":"Sensing contacts throughout the entire finger is an highly valuable capability for a robots (and humans) when carrying manipulation tasks,  as  vision-based  sensing  often  suffers  from  occlusions  or inaccurate  estimations. Current tactile sensors suffer from one of two drawbacks: low resolution readings, or a limited contact measurement area. We propose a finger-shaped optical sensor that has the shape of a finger and can sense contacts on any location of its surface. Our experiments show that the sensor can effectively capture such contacts throughout its surface.\n\n{% raw %}\n<img width=\"100%\" src=\"cover.jpg\" title=\"The GelTip Optical Tactile Sensor\" style=\"max-width:50rem; margin:3rem auto 1rem;\"> \n{% endraw %}\n\nIn figure above a plastic strawberry is being grasped by a parallel gripper equipped with two of the proposed GelTip sensors, with the corresponding imprint highlighted in the obtained tactile image (in gray-scale). As it can be seen, the sensor clearly capture the strawberry texture.\n\n## Materials\nIn the table bellow, the necessary STL files for 3D printing the GelTip sensor are provided. STL files are also provided for printing the 5 objects dataset and support mount, used in the Contact Localisation experiment. Please refer to the paper for more instructions on how to build the sensor or experiments details.\n\n| DESCRIPTION           |  FILE    |\n|-----------------------|:--------:|\n| **3D printable (STL) files of the GelSight sensor and mold.** | ** [geltip2020_parts.zip](geltip2020_parts.zip) ** |\n| **3D printable (STL) files of the objects and mount used in the contact localisation experiment.** | ** [objects_dataset.zip](objects_dataset.zip) ** |\n\n## Sensor projective model \nWe derive the protective function that maps pixels in the image space into points on the sensor surface, as illustrated in the picture below. The two shown rays intersect the sensor surface in the spherical region, in red, and the cylindrical region, in blue; and each ray intersects three relevant points: the frame of reference origin, a point in the sensor surface and the corresponding projected point in the image plane.\n\n{% raw %}\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"projective_model.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n{% endraw %}\n\n## Evaluation\n\n\n{% raw %}\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"exp_contact_detection.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n{% endraw %}\n\nTwo GelTip sensors are installed on a robotic actuator and a 3D printed mount that holds a small 3D printed object placed on top of a wooden block. The actuator moves in small increments and collects tactile images annotated with the known contact positions. We then implement an image-subtraction based algorithm to localise such contacts in image space. Then, using the projective model the localised contact points are projected into world coordinates. The errors between the true and predicted positions, measured as the Euclidean distance, are then assessed.\n\n{% raw %}\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"exp_world_blocks.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n{% endraw %}\nTo demonstrate the potential of all-around sensing, in the context of manipulation/grasping tasks a Blocks World is carried. The robot actuator moves row by row, attempting to grasp each block. Two different policies are analysed: one in which the robot grasps each block randomly, and another in which touch feedback is used to adjust the initially random grasp (the video above shows the latter). The experiment shows that when using touch feedback the robot grasps all the blocks successfully, even with the initial uncertainty.\n\nFor more information about the GelTip sensor, its fabrication process, and the executed experiments, checkout the papers referenced bellow.\n\n\n\n\n\n### References\n[^1]: Daniel Fernandes Gomes, Zhonglin Lin and Shan Luo, \"Blocks World of Touch: Exploiting the advantages of all-around finger sensing in robot grasping\", Frontiers in Robotics and AI 7 (2020). **[10.3389/frobt.2020.541661](http://doi.org/10.3389/frobt.2020.541661)**\n[^2]: Daniel Fernandes Gomes, Zhonglin Lin and Shan Luo, \"GelSight Simulation for Sim2Real Learning\", ViTac Workshop ICRA 2020. **[paper](http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2020/05/ICRA2020ViTac_paper_5.pdf)**","source":"_posts/geltip.md","raw":"title: 'GelTip: A Finger-shaped Optical Tactile Sensor for Robotic Manipulation'\nauthor: danfergo\ntags:\n  - touch\n  - featured\ncategories: []\ncover: cover.jpg\ndate: 2020-03-07 20:08:00\n---\nSensing contacts throughout the entire finger is an highly valuable capability for a robots (and humans) when carrying manipulation tasks,  as  vision-based  sensing  often  suffers  from  occlusions  or inaccurate  estimations. Current tactile sensors suffer from one of two drawbacks: low resolution readings, or a limited contact measurement area. We propose a finger-shaped optical sensor that has the shape of a finger and can sense contacts on any location of its surface. Our experiments show that the sensor can effectively capture such contacts throughout its surface.\n\n{% raw %}\n<img width=\"100%\" src=\"cover.jpg\" title=\"The GelTip Optical Tactile Sensor\" style=\"max-width:50rem; margin:3rem auto 1rem;\"> \n{% endraw %}\n\nIn figure above a plastic strawberry is being grasped by a parallel gripper equipped with two of the proposed GelTip sensors, with the corresponding imprint highlighted in the obtained tactile image (in gray-scale). As it can be seen, the sensor clearly capture the strawberry texture.\n\n## Materials\nIn the table bellow, the necessary STL files for 3D printing the GelTip sensor are provided. STL files are also provided for printing the 5 objects dataset and support mount, used in the Contact Localisation experiment. Please refer to the paper for more instructions on how to build the sensor or experiments details.\n\n| DESCRIPTION           |  FILE    |\n|-----------------------|:--------:|\n| **3D printable (STL) files of the GelSight sensor and mold.** | ** [geltip2020_parts.zip](geltip2020_parts.zip) ** |\n| **3D printable (STL) files of the objects and mount used in the contact localisation experiment.** | ** [objects_dataset.zip](objects_dataset.zip) ** |\n\n## Sensor projective model \nWe derive the protective function that maps pixels in the image space into points on the sensor surface, as illustrated in the picture below. The two shown rays intersect the sensor surface in the spherical region, in red, and the cylindrical region, in blue; and each ray intersects three relevant points: the frame of reference origin, a point in the sensor surface and the corresponding projected point in the image plane.\n\n{% raw %}\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"projective_model.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n{% endraw %}\n\n## Evaluation\n\n\n{% raw %}\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"exp_contact_detection.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n{% endraw %}\n\nTwo GelTip sensors are installed on a robotic actuator and a 3D printed mount that holds a small 3D printed object placed on top of a wooden block. The actuator moves in small increments and collects tactile images annotated with the known contact positions. We then implement an image-subtraction based algorithm to localise such contacts in image space. Then, using the projective model the localised contact points are projected into world coordinates. The errors between the true and predicted positions, measured as the Euclidean distance, are then assessed.\n\n{% raw %}\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"exp_world_blocks.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n{% endraw %}\nTo demonstrate the potential of all-around sensing, in the context of manipulation/grasping tasks a Blocks World is carried. The robot actuator moves row by row, attempting to grasp each block. Two different policies are analysed: one in which the robot grasps each block randomly, and another in which touch feedback is used to adjust the initially random grasp (the video above shows the latter). The experiment shows that when using touch feedback the robot grasps all the blocks successfully, even with the initial uncertainty.\n\nFor more information about the GelTip sensor, its fabrication process, and the executed experiments, checkout the papers referenced bellow.\n\n\n\n\n\n### References\n[^1]: Daniel Fernandes Gomes, Zhonglin Lin and Shan Luo, \"Blocks World of Touch: Exploiting the advantages of all-around finger sensing in robot grasping\", Frontiers in Robotics and AI 7 (2020). **[10.3389/frobt.2020.541661](http://doi.org/10.3389/frobt.2020.541661)**\n[^2]: Daniel Fernandes Gomes, Zhonglin Lin and Shan Luo, \"GelSight Simulation for Sim2Real Learning\", ViTac Workshop ICRA 2020. **[paper](http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2020/05/ICRA2020ViTac_paper_5.pdf)**","slug":"geltip","published":1,"updated":"2021-01-20T01:09:31.451Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckk4qbnrb000ghuvpcwfq12pw","content":"<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css\"><p>Sensing contacts throughout the entire finger is an highly valuable capability for a robots (and humans) when carrying manipulation tasks,  as  vision-based  sensing  often  suffers  from  occlusions  or inaccurate  estimations. Current tactile sensors suffer from one of two drawbacks: low resolution readings, or a limited contact measurement area. We propose a finger-shaped optical sensor that has the shape of a finger and can sense contacts on any location of its surface. Our experiments show that the sensor can effectively capture such contacts throughout its surface.</p>\n\n<img width=\"100%\" src=\"cover.jpg\" title=\"The GelTip Optical Tactile Sensor\" style=\"max-width:50rem; margin:3rem auto 1rem;\"> \n\n<p>In figure above a plastic strawberry is being grasped by a parallel gripper equipped with two of the proposed GelTip sensors, with the corresponding imprint highlighted in the obtained tactile image (in gray-scale). As it can be seen, the sensor clearly capture the strawberry texture.</p>\n<h2 id=\"Materials\"><a href=\"#Materials\" class=\"headerlink\" title=\"Materials\"></a>Materials</h2><p>In the table bellow, the necessary STL files for 3D printing the GelTip sensor are provided. STL files are also provided for printing the 5 objects dataset and support mount, used in the Contact Localisation experiment. Please refer to the paper for more instructions on how to build the sensor or experiments details.</p>\n<table>\n<thead>\n<tr>\n<th>DESCRIPTION</th>\n<th style=\"text-align:center\">FILE</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>3D printable (STL) files of the GelSight sensor and mold.</strong></td>\n<td style=\"text-align:center\"><strong> <a href=\"geltip2020_parts.zip\">geltip2020_parts.zip</a> </strong></td>\n</tr>\n<tr>\n<td><strong>3D printable (STL) files of the objects and mount used in the contact localisation experiment.</strong></td>\n<td style=\"text-align:center\"><strong> <a href=\"objects_dataset.zip\">objects_dataset.zip</a> </strong></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Sensor-projective-model\"><a href=\"#Sensor-projective-model\" class=\"headerlink\" title=\"Sensor projective model\"></a>Sensor projective model</h2><p>We derive the protective function that maps pixels in the image space into points on the sensor surface, as illustrated in the picture below. The two shown rays intersect the sensor surface in the spherical region, in red, and the cylindrical region, in blue; and each ray intersects three relevant points: the frame of reference origin, a point in the sensor surface and the corresponding projected point in the image plane.</p>\n\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"projective_model.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n\n<h2 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h2>\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"exp_contact_detection.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n\n<p>Two GelTip sensors are installed on a robotic actuator and a 3D printed mount that holds a small 3D printed object placed on top of a wooden block. The actuator moves in small increments and collects tactile images annotated with the known contact positions. We then implement an image-subtraction based algorithm to localise such contacts in image space. Then, using the projective model the localised contact points are projected into world coordinates. The errors between the true and predicted positions, measured as the Euclidean distance, are then assessed.</p>\n\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"exp_world_blocks.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n\n<p>To demonstrate the potential of all-around sensing, in the context of manipulation/grasping tasks a Blocks World is carried. The robot actuator moves row by row, attempting to grasp each block. Two different policies are analysed: one in which the robot grasps each block randomly, and another in which touch feedback is used to adjust the initially random grasp (the video above shows the latter). The experiment shows that when using touch feedback the robot grasps all the blocks successfully, even with the initial uncertainty.</p>\n<p>For more information about the GelTip sensor, its fabrication process, and the executed experiments, checkout the papers referenced bellow.</p>\n<h3 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h3><div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style: none; padding-left: 0; margin-left: 40px\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">1.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">Daniel Fernandes Gomes, Zhonglin Lin and Shan Luo, &quot;Blocks World of Touch: Exploiting the advantages of all-around finger sensing in robot grasping&quot;, Frontiers in Robotics and AI 7 (2020). <strong><a href=\"http://doi.org/10.3389/frobt.2020.541661\">10.3389/frobt.2020.541661</a></strong><a href=\"#fnref:1\" rev=\"footnote\"> ↩</a></span></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">2.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">Daniel Fernandes Gomes, Zhonglin Lin and Shan Luo, &quot;GelSight Simulation for Sim2Real Learning&quot;, ViTac Workshop ICRA 2020. <strong><a href=\"http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2020/05/ICRA2020ViTac_paper_5.pdf\">paper</a></strong><a href=\"#fnref:2\" rev=\"footnote\"> ↩</a></span></li></ol></div></div>","site":{"data":{}},"excerpt":"","more":"<p>Sensing contacts throughout the entire finger is an highly valuable capability for a robots (and humans) when carrying manipulation tasks,  as  vision-based  sensing  often  suffers  from  occlusions  or inaccurate  estimations. Current tactile sensors suffer from one of two drawbacks: low resolution readings, or a limited contact measurement area. We propose a finger-shaped optical sensor that has the shape of a finger and can sense contacts on any location of its surface. Our experiments show that the sensor can effectively capture such contacts throughout its surface.</p>\n\n<img width=\"100%\" src=\"cover.jpg\" title=\"The GelTip Optical Tactile Sensor\" style=\"max-width:50rem; margin:3rem auto 1rem;\"> \n\n<p>In figure above a plastic strawberry is being grasped by a parallel gripper equipped with two of the proposed GelTip sensors, with the corresponding imprint highlighted in the obtained tactile image (in gray-scale). As it can be seen, the sensor clearly capture the strawberry texture.</p>\n<h2 id=\"Materials\"><a href=\"#Materials\" class=\"headerlink\" title=\"Materials\"></a>Materials</h2><p>In the table bellow, the necessary STL files for 3D printing the GelTip sensor are provided. STL files are also provided for printing the 5 objects dataset and support mount, used in the Contact Localisation experiment. Please refer to the paper for more instructions on how to build the sensor or experiments details.</p>\n<table>\n<thead>\n<tr>\n<th>DESCRIPTION</th>\n<th style=\"text-align:center\">FILE</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>3D printable (STL) files of the GelSight sensor and mold.</strong></td>\n<td style=\"text-align:center\"><strong> <a href=\"geltip2020_parts.zip\">geltip2020_parts.zip</a> </strong></td>\n</tr>\n<tr>\n<td><strong>3D printable (STL) files of the objects and mount used in the contact localisation experiment.</strong></td>\n<td style=\"text-align:center\"><strong> <a href=\"objects_dataset.zip\">objects_dataset.zip</a> </strong></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Sensor-projective-model\"><a href=\"#Sensor-projective-model\" class=\"headerlink\" title=\"Sensor projective model\"></a>Sensor projective model</h2><p>We derive the protective function that maps pixels in the image space into points on the sensor surface, as illustrated in the picture below. The two shown rays intersect the sensor surface in the spherical region, in red, and the cylindrical region, in blue; and each ray intersects three relevant points: the frame of reference origin, a point in the sensor surface and the corresponding projected point in the image plane.</p>\n\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"projective_model.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n\n<h2 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h2>\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"exp_contact_detection.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n\n<p>Two GelTip sensors are installed on a robotic actuator and a 3D printed mount that holds a small 3D printed object placed on top of a wooden block. The actuator moves in small increments and collects tactile images annotated with the known contact positions. We then implement an image-subtraction based algorithm to localise such contacts in image space. Then, using the projective model the localised contact points are projected into world coordinates. The errors between the true and predicted positions, measured as the Euclidean distance, are then assessed.</p>\n\n<video width=\"600\" height=\"300\" controls>\n  <source src=\"exp_world_blocks.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.</video>\n\n<p>To demonstrate the potential of all-around sensing, in the context of manipulation/grasping tasks a Blocks World is carried. The robot actuator moves row by row, attempting to grasp each block. Two different policies are analysed: one in which the robot grasps each block randomly, and another in which touch feedback is used to adjust the initially random grasp (the video above shows the latter). The experiment shows that when using touch feedback the robot grasps all the blocks successfully, even with the initial uncertainty.</p>\n<p>For more information about the GelTip sensor, its fabrication process, and the executed experiments, checkout the papers referenced bellow.</p>\n<h3 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h3><div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style: none; padding-left: 0; margin-left: 40px\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">1.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">Daniel Fernandes Gomes, Zhonglin Lin and Shan Luo, &quot;Blocks World of Touch: Exploiting the advantages of all-around finger sensing in robot grasping&quot;, Frontiers in Robotics and AI 7 (2020). <strong><a href=\"http://doi.org/10.3389/frobt.2020.541661\">10.3389/frobt.2020.541661</a></strong><a href=\"#fnref:1\" rev=\"footnote\"> ↩</a></span></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">2.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">Daniel Fernandes Gomes, Zhonglin Lin and Shan Luo, &quot;GelSight Simulation for Sim2Real Learning&quot;, ViTac Workshop ICRA 2020. <strong><a href=\"http://wordpress.csc.liv.ac.uk/smartlab/wp-content/uploads/sites/5/2020/05/ICRA2020ViTac_paper_5.pdf\">paper</a></strong><a href=\"#fnref:2\" rev=\"footnote\"> ↩</a></span></li></ol></div></div>"},{"title":"Hello World!","subtitle":"What should you and I expect from this blog?","alias":"2016/10/14/hello-world/index.html","date":"2016-10-14T03:07:00.000Z","_content":"Since my teenager years I’ve been using the internet quite a lot and, with that, learning many on many different topics. Such was only possible because of the large amount of educational content provided by “anonymous” people through blogs, forums and video channels. Probably because of that, I always felt admired by this spirit of free and open sharing and mutual help.\n\nFrom early on, I’ve found myself thinking about creating a blog or a video channel about “How to program” or something similar, that would allow me to give back what I’ve learned, but I always came across other great content providers that were making it much better than I would probably ever done.\n\nToday with my studies course reaching its end, my master thesis arriving and my attention focusing on Computer Vision, Artificial Intelligence and Robotics I feel that the uniqueness of information that I would be capable to produce is greater and time is right to, once for all, create such web page. Also, this will be most valuable for me by serving as an alive showcase of what i’m doing and my interests as well as an extra motivation to learn and explore more. (You may find that this is not a \tnovelty on my life by reading my About page.)\n\nSo, in this blog you may expect to find content about AI, CV, Robotics and other computer related topics. These could appear in two types of posts: experiments/toy projects or supper summaries of “complex” subjects that I find possible to compress without lose to much. It is also possible that I explore here some of my philosophical ideas that often lie between natural intelligence/consciousnesses and the artificial counterpart. Finally, I may be sharing here some of the 5 minutes ideas that I have through the day and that I would like to work on if I had the number of lives of a cat.\n\nThat’s all; Thanks!\nUntil the next post.\n\n{% raw %}\n<iframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/273662969&amp;color=000000&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false\"></iframe>\n{% endraw %}","source":"_posts/hello-world.md","raw":"title: Hello World!\nsubtitle: What should you and I expect from this blog?\nalias: 2016/10/14/hello-world/index.html\ntags:\n  - meta\ncategories: []\ndate: 2016-10-14 04:07:00\n---\nSince my teenager years I’ve been using the internet quite a lot and, with that, learning many on many different topics. Such was only possible because of the large amount of educational content provided by “anonymous” people through blogs, forums and video channels. Probably because of that, I always felt admired by this spirit of free and open sharing and mutual help.\n\nFrom early on, I’ve found myself thinking about creating a blog or a video channel about “How to program” or something similar, that would allow me to give back what I’ve learned, but I always came across other great content providers that were making it much better than I would probably ever done.\n\nToday with my studies course reaching its end, my master thesis arriving and my attention focusing on Computer Vision, Artificial Intelligence and Robotics I feel that the uniqueness of information that I would be capable to produce is greater and time is right to, once for all, create such web page. Also, this will be most valuable for me by serving as an alive showcase of what i’m doing and my interests as well as an extra motivation to learn and explore more. (You may find that this is not a \tnovelty on my life by reading my About page.)\n\nSo, in this blog you may expect to find content about AI, CV, Robotics and other computer related topics. These could appear in two types of posts: experiments/toy projects or supper summaries of “complex” subjects that I find possible to compress without lose to much. It is also possible that I explore here some of my philosophical ideas that often lie between natural intelligence/consciousnesses and the artificial counterpart. Finally, I may be sharing here some of the 5 minutes ideas that I have through the day and that I would like to work on if I had the number of lives of a cat.\n\nThat’s all; Thanks!\nUntil the next post.\n\n{% raw %}\n<iframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/273662969&amp;color=000000&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false\"></iframe>\n{% endraw %}","slug":"hello-world","published":1,"updated":"2020-05-31T00:20:12.652Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckk4qbnrc000hhuvp4w175hag","content":"<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css\"><p>Since my teenager years I’ve been using the internet quite a lot and, with that, learning many on many different topics. Such was only possible because of the large amount of educational content provided by “anonymous” people through blogs, forums and video channels. Probably because of that, I always felt admired by this spirit of free and open sharing and mutual help.</p>\n<p>From early on, I’ve found myself thinking about creating a blog or a video channel about “How to program” or something similar, that would allow me to give back what I’ve learned, but I always came across other great content providers that were making it much better than I would probably ever done.</p>\n<p>Today with my studies course reaching its end, my master thesis arriving and my attention focusing on Computer Vision, Artificial Intelligence and Robotics I feel that the uniqueness of information that I would be capable to produce is greater and time is right to, once for all, create such web page. Also, this will be most valuable for me by serving as an alive showcase of what i’m doing and my interests as well as an extra motivation to learn and explore more. (You may find that this is not a     novelty on my life by reading my About page.)</p>\n<p>So, in this blog you may expect to find content about AI, CV, Robotics and other computer related topics. These could appear in two types of posts: experiments/toy projects or supper summaries of “complex” subjects that I find possible to compress without lose to much. It is also possible that I explore here some of my philosophical ideas that often lie between natural intelligence/consciousnesses and the artificial counterpart. Finally, I may be sharing here some of the 5 minutes ideas that I have through the day and that I would like to work on if I had the number of lives of a cat.</p>\n<p>That’s all; Thanks!<br>Until the next post.</p>\n\n<iframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/273662969&amp;color=000000&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false\"></iframe>\n","site":{"data":{}},"excerpt":"","more":"<p>Since my teenager years I’ve been using the internet quite a lot and, with that, learning many on many different topics. Such was only possible because of the large amount of educational content provided by “anonymous” people through blogs, forums and video channels. Probably because of that, I always felt admired by this spirit of free and open sharing and mutual help.</p>\n<p>From early on, I’ve found myself thinking about creating a blog or a video channel about “How to program” or something similar, that would allow me to give back what I’ve learned, but I always came across other great content providers that were making it much better than I would probably ever done.</p>\n<p>Today with my studies course reaching its end, my master thesis arriving and my attention focusing on Computer Vision, Artificial Intelligence and Robotics I feel that the uniqueness of information that I would be capable to produce is greater and time is right to, once for all, create such web page. Also, this will be most valuable for me by serving as an alive showcase of what i’m doing and my interests as well as an extra motivation to learn and explore more. (You may find that this is not a     novelty on my life by reading my About page.)</p>\n<p>So, in this blog you may expect to find content about AI, CV, Robotics and other computer related topics. These could appear in two types of posts: experiments/toy projects or supper summaries of “complex” subjects that I find possible to compress without lose to much. It is also possible that I explore here some of my philosophical ideas that often lie between natural intelligence/consciousnesses and the artificial counterpart. Finally, I may be sharing here some of the 5 minutes ideas that I have through the day and that I would like to work on if I had the number of lives of a cat.</p>\n<p>That’s all; Thanks!<br>Until the next post.</p>\n\n<iframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/273662969&amp;color=000000&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false\"></iframe>\n"}],"PostAsset":[{"_id":"source/_posts/garmnet/cover.png","slug":"cover.png","post":"ckk4qbnqu0001huvpgr5ubv9w","modified":0,"renderable":0},{"_id":"source/_posts/garmnet/img_spacial_constraint.png","slug":"img_spacial_constraint.png","post":"ckk4qbnqu0001huvpgr5ubv9w","modified":0,"renderable":0},{"_id":"source/_posts/clevo-p65xrp-inphtech-p600-g-on-linux/clevo-linux.jpg","slug":"clevo-linux.jpg","post":"ckk4qbnqy0003huvp22tsg6yq","modified":0,"renderable":0},{"_id":"source/_posts/gelsight-simulation/cover.png","slug":"cover.png","post":"ckk4qbnr20006huvphnvcgdiv","modified":0,"renderable":0},{"_id":"source/_posts/gelsight-simulation/data_collection.webm","slug":"data_collection.webm","post":"ckk4qbnr20006huvphnvcgdiv","modified":0,"renderable":0},{"_id":"source/_posts/gelsight-simulation/envs.png","slug":"envs.png","post":"ckk4qbnr20006huvphnvcgdiv","modified":0,"renderable":0},{"_id":"source/_posts/gelsight-simulation/heightmap.jpg","slug":"heightmap.jpg","post":"ckk4qbnr20006huvphnvcgdiv","modified":0,"renderable":0},{"_id":"source/_posts/gelsight-simulation/object_set.jpg","slug":"object_set.jpg","post":"ckk4qbnr20006huvphnvcgdiv","modified":0,"renderable":0},{"_id":"source/_posts/gelsight-simulation/samples.png","slug":"samples.png","post":"ckk4qbnr20006huvphnvcgdiv","modified":0,"renderable":0},{"_id":"source/_posts/gelsight-simulation/samples_qualitative.jpg","slug":"samples_qualitative.jpg","post":"ckk4qbnr20006huvphnvcgdiv","modified":0,"renderable":0},{"_id":"source/_posts/gelsight-simulation/texture_augmented.jpg","slug":"texture_augmented.jpg","post":"ckk4qbnr20006huvphnvcgdiv","modified":0,"renderable":0},{"_id":"source/_posts/gelsight-simulation/texture_perturbances.jpg","slug":"texture_perturbances.jpg","post":"ckk4qbnr20006huvphnvcgdiv","modified":0,"renderable":0},{"_id":"source/_posts/gelsight-simulation/vitac2019workshop.jpg","slug":"vitac2019workshop.jpg","post":"ckk4qbnr20006huvphnvcgdiv","modified":0,"renderable":0},{"_id":"source/_posts/geltip/cover.jpg","slug":"cover.jpg","post":"ckk4qbnrb000ghuvpcwfq12pw","modified":0,"renderable":0},{"_id":"source/_posts/geltip/exp_contact_detection.mp4","slug":"exp_contact_detection.mp4","post":"ckk4qbnrb000ghuvpcwfq12pw","modified":0,"renderable":0},{"_id":"source/_posts/geltip/exp_world_blocks.mp4","slug":"exp_world_blocks.mp4","post":"ckk4qbnrb000ghuvpcwfq12pw","modified":0,"renderable":0},{"_id":"source/_posts/geltip/geltip2020_parts.zip","slug":"geltip2020_parts.zip","post":"ckk4qbnrb000ghuvpcwfq12pw","modified":0,"renderable":0},{"_id":"source/_posts/geltip/objects_dataset.zip","slug":"objects_dataset.zip","post":"ckk4qbnrb000ghuvpcwfq12pw","modified":0,"renderable":0},{"_id":"source/_posts/geltip/projective_model.mp4","slug":"projective_model.mp4","post":"ckk4qbnrb000ghuvpcwfq12pw","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"ckk4qbnqu0001huvpgr5ubv9w","tag_id":"ckk4qbnqz0004huvpdl8jenhj","_id":"ckk4qbnr40009huvp2ku6b3lt"},{"post_id":"ckk4qbnqu0001huvpgr5ubv9w","tag_id":"ckk4qbnr30007huvpfb5yhzd8","_id":"ckk4qbnr4000ahuvp9aw8blo6"},{"post_id":"ckk4qbnqy0003huvp22tsg6yq","tag_id":"ckk4qbnr30008huvp3zs05eot","_id":"ckk4qbnr5000chuvp5abd910x"},{"post_id":"ckk4qbnr20006huvphnvcgdiv","tag_id":"ckk4qbnqz0004huvpdl8jenhj","_id":"ckk4qbnr6000ehuvp0ju4187j"},{"post_id":"ckk4qbnr20006huvphnvcgdiv","tag_id":"ckk4qbnr5000dhuvp8xfm0weo","_id":"ckk4qbnr6000fhuvp6d14doqd"},{"post_id":"ckk4qbnrb000ghuvpcwfq12pw","tag_id":"ckk4qbnr5000dhuvp8xfm0weo","_id":"ckk4qbnre000ihuvpeyre5blg"},{"post_id":"ckk4qbnrb000ghuvpcwfq12pw","tag_id":"ckk4qbnqz0004huvpdl8jenhj","_id":"ckk4qbnre000khuvph29d01m1"},{"post_id":"ckk4qbnrc000hhuvp4w175hag","tag_id":"ckk4qbnre000jhuvp8fyp9dpw","_id":"ckk4qbnrf000lhuvphlvlbfkq"}],"Tag":[{"name":"featured","_id":"ckk4qbnqz0004huvpdl8jenhj"},{"name":"vision","_id":"ckk4qbnr30007huvpfb5yhzd8"},{"name":"linux","_id":"ckk4qbnr30008huvp3zs05eot"},{"name":"touch","_id":"ckk4qbnr5000dhuvp8xfm0weo"},{"name":"meta","_id":"ckk4qbnre000jhuvp8fyp9dpw"}]}}